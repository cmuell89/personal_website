<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[Dylan La Com' RSS feed]]></title><description><![CDATA[Carl Mueller's website]]></description><link>https://www.carl-mueller.com</link><generator>GatsbyJS</generator><lastBuildDate>Mon, 25 Jan 2021 20:12:18 GMT</lastBuildDate><item><title><![CDATA[Talking Robotics Seminar Presentation]]></title><link>https://www.carl-mueller.com/research/talking-robotics/</link><guid isPermaLink="false">https://www.carl-mueller.com/research/talking-robotics/</guid><pubDate>Mon, 25 Jan 2021 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;I recently gave a virtual seminar presentation on the state of my thesis work. This includes prior work on CC-LfD and ARC-LfD, my current work on constrained motion planning, and future human-subjects studies coming down the pipeline. &lt;/p&gt;
&lt;p&gt;The seminar series, called &lt;a href=&quot;https://talking-robotics.github.io/&quot;&gt;Talking Robotics&lt;/a&gt;, is hosted by a group of students and post-docs, one of whom I met at the AAAI Doctoral Consortium. It is a great opportunity for current students, post-docs, and faculty to continue to advertise their research and network with their peers, especially in light of the COVID19 pandemic.&lt;/p&gt;
&lt;p&gt;The landing page for my presentation can be found &lt;a href=&quot;https://talking-robotics.github.io/session_details/carl.html&quot;&gt;HERE&lt;/a&gt;, but be sure to explore all the other great talks on the website!&lt;/p&gt;
&lt;p&gt;You can also view my presentation directly below:&lt;/p&gt;
&lt;p&gt;&lt;div class=&quot;gatsby-resp-iframe-wrapper&quot; style=&quot;padding-bottom: 50%; position: relative; height: 0; overflow: hidden; margin-bottom: 1.0725rem&quot; &gt; &lt;div&gt;&lt;iframe src=&quot;https://www.youtube.com/embed/0PbUMQcFEUs&quot; style=&quot; position: absolute; top: 0; left: 0; width: 100%; height: 100%; &quot;&gt;&lt;/iframe&gt;&lt;/div&gt; &lt;/div&gt;&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Augmented Reality for Constrained Learning from Demonstration]]></title><link>https://www.carl-mueller.com/research/arc-lfd/</link><guid isPermaLink="false">https://www.carl-mueller.com/research/arc-lfd/</guid><pubDate>Fri, 25 Sep 2020 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;We recently submitted a paper to ICRA 2021 that utilizes Augmented Reality to visualize models learned through user demonstration as well as an interface to add/remove/edit constraints on those models.&lt;/p&gt;
&lt;h2&gt;Overview&lt;/h2&gt;
&lt;p&gt;ARC-LfD is an augmented reality interfaces that enables users to visualize a learned keyframe model from CC-LfD. It accomplishes this through visualizations of sampled keyframes that the robot will follow as waypoints, as well as visualizations of constraints assigned to keyframes. In the below image, you can see sequences of expected robot behavior through keyframes, as well as visualizations of applied constraints.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 350px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/8ceab612f4922612b99dae5fb0f41598/e2c35/ar_viz_fig.jpg&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 55.68181818181818%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAALABQDASIAAhEBAxEB/8QAGAAAAwEBAAAAAAAAAAAAAAAAAAIFAQT/xAAVAQEBAAAAAAAAAAAAAAAAAAAAAv/aAAwDAQACEAMQAAAB50yhNIUg/8QAGxAAAgMAAwAAAAAAAAAAAAAAAQIAAxMRIiP/2gAIAQEAAQUCCJx4wVCJ2vCjfNDP/8QAFREBAQAAAAAAAAAAAAAAAAAAARD/2gAIAQMBAT8BJ//EABURAQEAAAAAAAAAAAAAAAAAABAR/9oACAECAQE/Aaf/xAAcEAADAAEFAAAAAAAAAAAAAAAAARECITFCUXH/2gAIAQEABj8CubnhyZsZXUT7LD//xAAbEAACAwADAAAAAAAAAAAAAAAAAREhMVFh0f/aAAgBAQABPyHUSa5ENCSZA6vR0FklTWlm0ss//9oADAMBAAIAAwAAABDD/wD/xAAWEQEBAQAAAAAAAAAAAAAAAAAAARH/2gAIAQMBAT8Qhkf/xAAWEQADAAAAAAAAAAAAAAAAAAABEFH/2gAIAQIBAT8QFL//xAAeEAEBAAICAgMAAAAAAAAAAAABEQAxIUFRYXGBkf/aAAgBAQABPxB6VZwIIdHn6y6SwtTb4dZeg9zvO/eC5qQ0WYWGtp5sCfGWK2xFL+Z//9k=&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;ar viz fig&quot;
        title=&quot;ar viz fig&quot;
        src=&quot;/static/8ceab612f4922612b99dae5fb0f41598/70ebb/ar_viz_fig.jpg&quot;
        srcset=&quot;/static/8ceab612f4922612b99dae5fb0f41598/f71f9/ar_viz_fig.jpg 88w,
/static/8ceab612f4922612b99dae5fb0f41598/e52aa/ar_viz_fig.jpg 175w,
/static/8ceab612f4922612b99dae5fb0f41598/70ebb/ar_viz_fig.jpg 350w,
/static/8ceab612f4922612b99dae5fb0f41598/7349d/ar_viz_fig.jpg 525w,
/static/8ceab612f4922612b99dae5fb0f41598/29d31/ar_viz_fig.jpg 700w,
/static/8ceab612f4922612b99dae5fb0f41598/e2c35/ar_viz_fig.jpg 2399w&quot;
        sizes=&quot;(max-width: 350px) 100vw, 350px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;figcaption&gt;AR visualizations of expected robot skill execution and various constraints.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;ARC-LfD also enables users to edit constraints through menus that allow for constraint specific parameter editing. Users can also add and remove the application of constraints by selecting an end-effector keyframe.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 350px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/ef2a9d5f376660bbcc13aebc12da35e7/b0c0e/ar_interaction_fig.jpg&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 82.95454545454545%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAARABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAAAAQFAv/EABYBAQEBAAAAAAAAAAAAAAAAAAABAv/aAAwDAQACEAMQAAABnpirMlpCWgXkXP8A/8QAHBAAAQQDAQAAAAAAAAAAAAAAAQACAxESEyEy/9oACAEBAAEFAtz8Jber6JjiZyH7rLUfYX//xAAVEQEBAAAAAAAAAAAAAAAAAAAQUf/aAAgBAwEBPwEh/8QAFBEBAAAAAAAAAAAAAAAAAAAAIP/aAAgBAgEBPwEf/8QAHhAAAQEJAAAAAAAAAAAAAAAAADEBAhAgITKRweH/2gAIAQEABj8CuabhwWgjuJP/xAAgEAEAAgECBwAAAAAAAAAAAAABADERIUEQUWFxkbHR/9oACAEBAAE/Ieot4tsTR8IoRmnnPtYRIYU0xpFIlkVlPY9cH//aAAwDAQACAAMAAAAQBOhB/8QAGBEAAgMAAAAAAAAAAAAAAAAAAAEQESH/2gAIAQMBAT8QsyDP/8QAFxEAAwEAAAAAAAAAAAAAAAAAAAEREP/aAAgBAgEBPxBqke//xAAfEAEAAgIBBQEAAAAAAAAAAAABABEhMWFRcYGRsfD/2gAIAQEAAT8QGxe2ViRa0kBLT7cxNoUKgwBtLVJTzxETjqhdC0j2Zo1zN/n5PxuiaPef/9k=&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;ar interaction fig&quot;
        title=&quot;ar interaction fig&quot;
        src=&quot;/static/ef2a9d5f376660bbcc13aebc12da35e7/70ebb/ar_interaction_fig.jpg&quot;
        srcset=&quot;/static/ef2a9d5f376660bbcc13aebc12da35e7/f71f9/ar_interaction_fig.jpg 88w,
/static/ef2a9d5f376660bbcc13aebc12da35e7/e52aa/ar_interaction_fig.jpg 175w,
/static/ef2a9d5f376660bbcc13aebc12da35e7/70ebb/ar_interaction_fig.jpg 350w,
/static/ef2a9d5f376660bbcc13aebc12da35e7/7349d/ar_interaction_fig.jpg 525w,
/static/ef2a9d5f376660bbcc13aebc12da35e7/29d31/ar_interaction_fig.jpg 700w,
/static/ef2a9d5f376660bbcc13aebc12da35e7/b0c0e/ar_interaction_fig.jpg 1622w&quot;
        sizes=&quot;(max-width: 350px) 100vw, 350px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;figcaption&gt;Editing menus of ARC-LfD to reparameterize constraints applied to the CC-LfD model&lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;Below is a video describing the ARC-LfD system:&lt;/p&gt;
&lt;p&gt;&lt;div class=&quot;gatsby-resp-iframe-wrapper&quot; style=&quot;padding-bottom: 50%; position: relative; height: 0; overflow: hidden; margin-bottom: 1.0725rem&quot; &gt; &lt;div&gt;&lt;iframe src=&quot;https://www.youtube.com/embed/X0H7rszq-QA&quot; style=&quot; position: absolute; top: 0; left: 0; width: 100%; height: 100%; &quot;&gt;&lt;/iframe&gt;&lt;/div&gt; &lt;/div&gt;&lt;/p&gt;
&lt;h2&gt;Interaction Flow&lt;/h2&gt;
&lt;p&gt;The use interaction flow for the ARC-LfD:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Recording of demonstration trajectories and constraints&lt;/li&gt;
&lt;li&gt;Visualization of the learned model&lt;/li&gt;
&lt;li&gt;Constraint editing/application&lt;/li&gt;
&lt;li&gt;Relearning of Constrained Keyframe Model via CC-LfD algorithm&lt;/li&gt;
&lt;li&gt;Model acceptance and resulting skill execution&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The iterative aspect is represented by the looping of steps 2, 3, and 4.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 350px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/5e3618a3e05d8aef0f7cc66302a2ff1c/0d333/ARC-LfD_Editing_Flowchart.jpg&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 95.45454545454545%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAATABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAAAAIBBf/EABUBAQEAAAAAAAAAAAAAAAAAAAAB/9oADAMBAAIQAxAAAAHsKgsJgUD/xAAYEAADAQEAAAAAAAAAAAAAAAAAAREQIf/aAAgBAQABBQJ6zpSIiz//xAAUEQEAAAAAAAAAAAAAAAAAAAAg/9oACAEDAQE/AR//xAAUEQEAAAAAAAAAAAAAAAAAAAAg/9oACAECAQE/AR//xAAVEAEBAAAAAAAAAAAAAAAAAAAgIf/aAAgBAQAGPwIU/wD/xAAbEAADAQEAAwAAAAAAAAAAAAAAAREhMWFxof/aAAgBAQABPyHU2DbS78KaQoekex4EUyES4f/aAAwDAQACAAMAAAAQk9DD/8QAFhEAAwAAAAAAAAAAAAAAAAAAABAR/9oACAEDAQE/ECP/xAAWEQEBAQAAAAAAAAAAAAAAAAARABD/2gAIAQIBAT8QI3//xAAcEAEAAgMBAQEAAAAAAAAAAAABABEhMVFBYZH/2gAIAQEAAT8Qz0VM2QvmhtdvyBMIkWAW3VdlAuPK7AeCLKrL8iQRqBKFByf/2Q==&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;ARC LfD Editing Flowchart&quot;
        title=&quot;ARC LfD Editing Flowchart&quot;
        src=&quot;/static/5e3618a3e05d8aef0f7cc66302a2ff1c/70ebb/ARC-LfD_Editing_Flowchart.jpg&quot;
        srcset=&quot;/static/5e3618a3e05d8aef0f7cc66302a2ff1c/f71f9/ARC-LfD_Editing_Flowchart.jpg 88w,
/static/5e3618a3e05d8aef0f7cc66302a2ff1c/e52aa/ARC-LfD_Editing_Flowchart.jpg 175w,
/static/5e3618a3e05d8aef0f7cc66302a2ff1c/70ebb/ARC-LfD_Editing_Flowchart.jpg 350w,
/static/5e3618a3e05d8aef0f7cc66302a2ff1c/7349d/ARC-LfD_Editing_Flowchart.jpg 525w,
/static/5e3618a3e05d8aef0f7cc66302a2ff1c/29d31/ARC-LfD_Editing_Flowchart.jpg 700w,
/static/5e3618a3e05d8aef0f7cc66302a2ff1c/0d333/ARC-LfD_Editing_Flowchart.jpg 1175w&quot;
        sizes=&quot;(max-width: 350px) 100vw, 350px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;figcaption&gt;The iterative interaction and model relearning flow diagram of ARC-LfD.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;h2&gt;Architecture&lt;/h2&gt;
&lt;p&gt;The ARC-LfD architecture consists of two major subsystems:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Augmented Reality Subsystem&lt;/li&gt;
&lt;li&gt;CC-LfD Subsystem&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These two subsystems communicate via the pub/sub architecture of the Robot Operating System (ROS). Any time the user updates constraints on the mode, the AR Subsystem sends those updates to the CC-LfD subsystem which produces a new model, and sends a representation back to the AR Subsystem to update the visualizations.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 350px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/e060700bb12ff3d76112dd9e51f5fffc/dbfa0/ARCLfD_Architecture.jpg&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 69.31818181818181%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAOABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAAAAIDBf/EABQBAQAAAAAAAAAAAAAAAAAAAAD/2gAMAwEAAhADEAAAAe1FSag//8QAGRABAAMBAQAAAAAAAAAAAAAAAQARMSEi/9oACAEBAAEFAmpyGIX5hn//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/AT//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/AT//xAAWEAADAAAAAAAAAAAAAAAAAAAQIDH/2gAIAQEABj8CSD//xAAZEAEAAwEBAAAAAAAAAAAAAAABABEhkaH/2gAIAQEAAT8huYut7FYmxWylY+4AxP/aAAwDAQACAAMAAAAQ8M//xAAWEQEBAQAAAAAAAAAAAAAAAAABAFH/2gAIAQMBAT8QDZv/xAAVEQEBAAAAAAAAAAAAAAAAAAAQIf/aAAgBAgEBPxCn/8QAGhABAQEAAwEAAAAAAAAAAAAAAREAITFBUf/aAAgBAQABPxBAKWdLMESA8VOcbiPHmkRfp0tAvSucADD5d//Z&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;ARCLfD Architecture&quot;
        title=&quot;ARCLfD Architecture&quot;
        src=&quot;/static/e060700bb12ff3d76112dd9e51f5fffc/70ebb/ARCLfD_Architecture.jpg&quot;
        srcset=&quot;/static/e060700bb12ff3d76112dd9e51f5fffc/f71f9/ARCLfD_Architecture.jpg 88w,
/static/e060700bb12ff3d76112dd9e51f5fffc/e52aa/ARCLfD_Architecture.jpg 175w,
/static/e060700bb12ff3d76112dd9e51f5fffc/70ebb/ARCLfD_Architecture.jpg 350w,
/static/e060700bb12ff3d76112dd9e51f5fffc/7349d/ARCLfD_Architecture.jpg 525w,
/static/e060700bb12ff3d76112dd9e51f5fffc/29d31/ARCLfD_Architecture.jpg 700w,
/static/e060700bb12ff3d76112dd9e51f5fffc/dbfa0/ARCLfD_Architecture.jpg 2467w&quot;
        sizes=&quot;(max-width: 350px) 100vw, 350px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;figcaption&gt;The iterative interaction and model relearning flow diagram of ARC-LfD.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;h2&gt;Future Work&lt;/h2&gt;
&lt;p&gt;Future work will consist of running human-subjects studies to test the efficacy of this system both objectively: the skill success percentage compared to traditional constraint application, and subjectively: do users display a preference using this interface?&lt;/p&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;&lt;bibliography&gt;&lt;bibliography&gt;&lt;/p&gt;</content:encoded></item><item><title><![CDATA[2020 AAAI, HRI Pioneers, COVID19]]></title><link>https://www.carl-mueller.com/posts/update-sep2020/</link><guid isPermaLink="false">https://www.carl-mueller.com/posts/update-sep2020/</guid><pubDate>Tue, 01 Sep 2020 22:00:00 GMT</pubDate><content:encoded>&lt;p&gt;This past February, I had the privliege to attend 2020 AAAI Conference in New York City. I was selected into the Doctoral Consortium, which serves as a workshop for promising PhD students in the greater AI community. I had a wonderful time meeting people from all walks of life and enjoying the world’s most cosmopolitan city. I got to sit in on all sorts of interesting talks on all sorts of topics in AI and Machine Learning. I even sawy Geoffrey Hinton, Yann LeCunn, and Yoshio Bengio recieve their Turing Awards. Pretty amazing!&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 350px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/7d3ebd0d391d1e89037d9fe052956684/67226/PosterSession.jpg&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 132.95454545454547%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAbABQDASIAAhEBAxEB/8QAGAAAAwEBAAAAAAAAAAAAAAAAAAQFAwL/xAAWAQEBAQAAAAAAAAAAAAAAAAABAAL/2gAMAwEAAhADEAAAAUc7eGWMWxtp6fBMiw3/xAAaEAADAAMBAAAAAAAAAAAAAAAAAQIDERIx/9oACAEBAAEFAmhwc0ieucqSFePXiyVuy7pm3I/f/8QAFBEBAAAAAAAAAAAAAAAAAAAAIP/aAAgBAwEBPwEf/8QAFBEBAAAAAAAAAAAAAAAAAAAAIP/aAAgBAgEBPwEf/8QAHRAAAQQCAwAAAAAAAAAAAAAAAQAQITECEUFhof/aAAgBAQAGPwJ8apRiN9KR4hNBbBaSo5b/xAAbEAEBAQADAQEAAAAAAAAAAAABEQAhMUFhcf/aAAgBAQABPyESDSnc41SnW8GIJYeA/m9EcYI0wqn2G+MPNZ24fldYVPWde//aAAwDAQACAAMAAAAQZBPw/8QAFhEBAQEAAAAAAAAAAAAAAAAAABEB/9oACAEDAQE/EImMV//EABcRAQEBAQAAAAAAAAAAAAAAAAARAVH/2gAIAQIBAT8QvF1Ef//EAB0QAQEBAQACAwEAAAAAAAAAAAERACExQWFxkdH/2gAIAQEAAT8QSBAaj9Z1Cjyng1CNDzkUzAv40IN7eID2nKYIOQPDlRwQSBydZi9IA08vdCea+sKnDJeT63ocT5ZqGxhv/9k=&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;PosterSession&quot;
        title=&quot;PosterSession&quot;
        src=&quot;/static/7d3ebd0d391d1e89037d9fe052956684/70ebb/PosterSession.jpg&quot;
        srcset=&quot;/static/7d3ebd0d391d1e89037d9fe052956684/f71f9/PosterSession.jpg 88w,
/static/7d3ebd0d391d1e89037d9fe052956684/e52aa/PosterSession.jpg 175w,
/static/7d3ebd0d391d1e89037d9fe052956684/70ebb/PosterSession.jpg 350w,
/static/7d3ebd0d391d1e89037d9fe052956684/7349d/PosterSession.jpg 525w,
/static/7d3ebd0d391d1e89037d9fe052956684/29d31/PosterSession.jpg 700w,
/static/7d3ebd0d391d1e89037d9fe052956684/67226/PosterSession.jpg 3072w&quot;
        sizes=&quot;(max-width: 350px) 100vw, 350px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;figcaption&gt;The poster session for the Doctoral Consortium.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;figure&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 350px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/77ab0596824f0bcfadf6052e275458c2/a0850/DrearyNYC.jpg&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 75%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAPABQDASIAAhEBAxEB/8QAFgABAQEAAAAAAAAAAAAAAAAABQAB/8QAFQEBAQAAAAAAAAAAAAAAAAAAAQD/2gAMAwEAAhADEAAAAUcDwW4Sj//EABgQAQEBAQEAAAAAAAAAAAAAAAECABIR/9oACAEBAAEFAupM3Bm4xfu6rCJ//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPwE//8QAFREBAQAAAAAAAAAAAAAAAAAAABH/2gAIAQIBAT8BV//EABgQAQADAQAAAAAAAAAAAAAAAAABMTIC/9oACAEBAAY/AtQ1DUKUvp//xAAbEAEAAgIDAAAAAAAAAAAAAAABABEhMUFxkf/aAAgBAQABPyF3pbSAHPvKd9TMczUDbL0k80E//9oADAMBAAIAAwAAABA7P//EABURAQEAAAAAAAAAAAAAAAAAAAAR/9oACAEDAQE/EIj/xAAVEQEBAAAAAAAAAAAAAAAAAAAQEf/aAAgBAgEBPxCj/8QAGxAAAwEBAAMAAAAAAAAAAAAAAREhADFRYbH/2gAIAQEAAT8QSgFzr+YgWEGVzTiYw1cBtkk+Jl2IoWsmPwCQ/U3/2Q==&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;DrearyNYC&quot;
        title=&quot;DrearyNYC&quot;
        src=&quot;/static/77ab0596824f0bcfadf6052e275458c2/70ebb/DrearyNYC.jpg&quot;
        srcset=&quot;/static/77ab0596824f0bcfadf6052e275458c2/f71f9/DrearyNYC.jpg 88w,
/static/77ab0596824f0bcfadf6052e275458c2/e52aa/DrearyNYC.jpg 175w,
/static/77ab0596824f0bcfadf6052e275458c2/70ebb/DrearyNYC.jpg 350w,
/static/77ab0596824f0bcfadf6052e275458c2/7349d/DrearyNYC.jpg 525w,
/static/77ab0596824f0bcfadf6052e275458c2/29d31/DrearyNYC.jpg 700w,
/static/77ab0596824f0bcfadf6052e275458c2/a0850/DrearyNYC.jpg 4096w&quot;
        sizes=&quot;(max-width: 350px) 100vw, 350px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;figcaption&gt;Shot from the rooftop of Google NYC. It was bitterly cold that weekend..&lt;/figcaption&gt;&lt;/figure&gt;
&lt;figure&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 350px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/8e7f2a53ddea34f66149ec199d9a7f31/a0850/PanPizza.jpg&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 75%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAPABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAABAACA//EABUBAQEAAAAAAAAAAAAAAAAAAAAC/9oADAMBAAIQAxAAAAEqi7iukWP/xAAaEAEBAAMBAQAAAAAAAAAAAAACAQADEhMx/9oACAEBAAEFAqDLx5SkbMS4wubEvv8A/8QAFhEBAQEAAAAAAAAAAAAAAAAAABEx/9oACAEDAQE/AdR//8QAFhEBAQEAAAAAAAAAAAAAAAAAABIx/9oACAECAQE/AcU//8QAHBAAAgICAwAAAAAAAAAAAAAAAAERIgIDEDFx/9oACAEBAAY/AmTh5JOx2LZKR274/8QAHBABAAMBAAMBAAAAAAAAAAAAAQARIUExUWGR/9oACAEBAAE/ITQ2slRVV11KeqMa7N1pcBnjx2J6i+v2f//aAAwDAQACAAMAAAAQu/8A/8QAFhEBAQEAAAAAAAAAAAAAAAAAAAER/9oACAEDAQE/EJTL/8QAGBEBAAMBAAAAAAAAAAAAAAAAARARITH/2gAIAQIBAT8QpwnY/wD/xAAbEAEBAAMBAQEAAAAAAAAAAAABEQAhMWGxQf/aAAgBAQABPxCmQaF2s8OOFRSU0UozhrNMMsdR+8y10GVPgYKwQWUvE9MGxaNaj4Z//9k=&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;PanPizza&quot;
        title=&quot;PanPizza&quot;
        src=&quot;/static/8e7f2a53ddea34f66149ec199d9a7f31/70ebb/PanPizza.jpg&quot;
        srcset=&quot;/static/8e7f2a53ddea34f66149ec199d9a7f31/f71f9/PanPizza.jpg 88w,
/static/8e7f2a53ddea34f66149ec199d9a7f31/e52aa/PanPizza.jpg 175w,
/static/8e7f2a53ddea34f66149ec199d9a7f31/70ebb/PanPizza.jpg 350w,
/static/8e7f2a53ddea34f66149ec199d9a7f31/7349d/PanPizza.jpg 525w,
/static/8e7f2a53ddea34f66149ec199d9a7f31/29d31/PanPizza.jpg 700w,
/static/8e7f2a53ddea34f66149ec199d9a7f31/a0850/PanPizza.jpg 4096w&quot;
        sizes=&quot;(max-width: 350px) 100vw, 350px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;figcaption&gt;Amazing Pan Pizza. NYC has the best pizza. The best!&lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;Missy also came to vist the weekend proceeding the conference. My parents also came down that weekend. My Uncle, Charlie Mueller PhD, took us to see Book of Mormon. It was truly hilarious and I hihgly recommend seeing it. Missy and I also saw the Lumineers in Brooklyn. The ongoing theme was the bitter cold. It was as cold as the East Coast gets.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 350px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/4055f883f751a48c4d1f673f5313441b/a0850/MissyInNYC.jpg&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 75%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAPABQDASIAAhEBAxEB/8QAGAAAAgMAAAAAAAAAAAAAAAAAAAMBBAX/xAAVAQEBAAAAAAAAAAAAAAAAAAAAAf/aAAwDAQACEAMQAAABWxkGYXyX/8QAGRABAAMBAQAAAAAAAAAAAAAAAgEDERIA/9oACAEBAAEFAisKnr07og5p6uCVn//EABYRAAMAAAAAAAAAAAAAAAAAAAEQEv/aAAgBAwEBPwGQv//EABcRAQADAAAAAAAAAAAAAAAAAAABERL/2gAIAQIBAT8B1K3/xAAaEAACAwEBAAAAAAAAAAAAAAAAAQIRIRIx/9oACAEBAAY/AqKjg+taPNKhjLUqR//EABoQAQADAQEBAAAAAAAAAAAAAAEAESExUUH/2gAIAQEAAT8hYV8IVGWvayCk1FXECz6yCIstzkSC8En/2gAMAwEAAgADAAAAENPv/8QAFxEBAQEBAAAAAAAAAAAAAAAAAREAIf/aAAgBAwEBPxAZqaTm/8QAFxEBAQEBAAAAAAAAAAAAAAAAAQAhMf/aAAgBAgEBPxBI7Adv/8QAGxABAQEAAwEBAAAAAAAAAAAAAREAMUGBIXH/2gAIAQEAAT8QKaQLOcmHTujvu+YzzNrwOWqQUcGfuIAMI+4Tx03QAokJv//Z&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;MissyInNYC&quot;
        title=&quot;MissyInNYC&quot;
        src=&quot;/static/4055f883f751a48c4d1f673f5313441b/70ebb/MissyInNYC.jpg&quot;
        srcset=&quot;/static/4055f883f751a48c4d1f673f5313441b/f71f9/MissyInNYC.jpg 88w,
/static/4055f883f751a48c4d1f673f5313441b/e52aa/MissyInNYC.jpg 175w,
/static/4055f883f751a48c4d1f673f5313441b/70ebb/MissyInNYC.jpg 350w,
/static/4055f883f751a48c4d1f673f5313441b/7349d/MissyInNYC.jpg 525w,
/static/4055f883f751a48c4d1f673f5313441b/29d31/MissyInNYC.jpg 700w,
/static/4055f883f751a48c4d1f673f5313441b/a0850/MissyInNYC.jpg 4096w&quot;
        sizes=&quot;(max-width: 350px) 100vw, 350px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;figcaption&gt;Walking the High Line with Missy in sub-zero temps!&lt;/figcaption&gt;&lt;/figure&gt;
&lt;figure&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 350px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/6769451079588740aabcd022b54ca88e/67226/TheHighLine.jpg&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 132.95454545454547%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAbABQDASIAAhEBAxEB/8QAGQAAAwEBAQAAAAAAAAAAAAAAAAECAwQF/8QAFQEBAQAAAAAAAAAAAAAAAAAAAQL/2gAMAwEAAhADEAAAAcOnzaTQxBtw0oZF/wD/xAAcEAACAgMBAQAAAAAAAAAAAAAAAgERAxIhBBP/2gAIAQEAAQUCwtWT08KL67TJZsbH0KIWKhVP/8QAGREAAwADAAAAAAAAAAAAAAAAAAERAhAS/9oACAEDAQE/AXjXTnUP/8QAFxEAAwEAAAAAAAAAAAAAAAAAABAREv/aAAgBAgEBPwGmlT//xAAaEAACAwEBAAAAAAAAAAAAAAAAAQIQMTNB/9oACAEBAAY/AkvGQrRNvK5xOUDlC8P/xAAgEAEAAgEEAgMAAAAAAAAAAAABABEhMVFx0UFhgZGh/9oACAEBAAE/IVdd6YBfmZ6g8tRIUR7luLQC2IvWab/N7nM+HuCrH0vcBtEKyOMif//aAAwDAQACAAMAAAAQHzF+/8QAFxEBAQEBAAAAAAAAAAAAAAAAAAERIf/aAAgBAwEBPxCETCdaf//EABcRAQEBAQAAAAAAAAAAAAAAAAEAESH/2gAIAQIBAT8QGCWp5Zv/xAAcEAADAAMBAQEAAAAAAAAAAAAAAREhMUFhkaH/2gAIAQEAAT8Q3oJeHN/g6gLTl45GNGvqWfBjRXEqHmAQVRvHvqpWR+CTNLhGkbCA2ppoZmVNzLKlp0//2Q==&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;TheHighLine&quot;
        title=&quot;TheHighLine&quot;
        src=&quot;/static/6769451079588740aabcd022b54ca88e/70ebb/TheHighLine.jpg&quot;
        srcset=&quot;/static/6769451079588740aabcd022b54ca88e/f71f9/TheHighLine.jpg 88w,
/static/6769451079588740aabcd022b54ca88e/e52aa/TheHighLine.jpg 175w,
/static/6769451079588740aabcd022b54ca88e/70ebb/TheHighLine.jpg 350w,
/static/6769451079588740aabcd022b54ca88e/7349d/TheHighLine.jpg 525w,
/static/6769451079588740aabcd022b54ca88e/29d31/TheHighLine.jpg 700w,
/static/6769451079588740aabcd022b54ca88e/67226/TheHighLine.jpg 3072w&quot;
        sizes=&quot;(max-width: 350px) 100vw, 350px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;figcaption&gt;View of the High Line in NYC.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;A month or so after the conference, I found out that I was also accepted into the HRI Pioneers workshop. This is a similar event as the AAAI Doctoral Consortium but more focused on an individual paper, and much more focused on my niche field of Human-Robot Interaction. Unfortunately, the conference was cancelled due to the COVID19 pandemic. I was supposed to go to Cambridge, UK so I was quite disappointed. What crazy times.&lt;/p&gt;
&lt;p&gt;Due to CU Boulder shutting down in the late Spring and through the Summer, I was fortunate to be able to go to California and live with Missy for the summer. We didn’t get to do much since we were respecting social distancing, but we did sneak a trip to South Lake Tahoe. I got some solid mountain biking in there, and throughout the summer. I may have also got a new mountain bike as well…But more importantly, it was really nice to be able be with Missy as we still do long-distance. &lt;/p&gt;
&lt;p&gt;&lt;figure&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 350px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/0021dbd702b4e8db3e12fd881f3daa86/a0850/Utah.jpg&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 75%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAPABQDASIAAhEBAxEB/8QAGAAAAwEBAAAAAAAAAAAAAAAAAAMEAgX/xAAVAQEBAAAAAAAAAAAAAAAAAAAAAv/aAAwDAQACEAMQAAABpVzNTTyYP//EABoQAAMAAwEAAAAAAAAAAAAAAAABAwISEzH/2gAIAQEAAQUCVJGdZo7CPTU//8QAFREBAQAAAAAAAAAAAAAAAAAAABH/2gAIAQMBAT8BiP/EABURAQEAAAAAAAAAAAAAAAAAAAAS/9oACAECAQE/AaU//8QAGhAAAgIDAAAAAAAAAAAAAAAAAAEgIjEyof/aAAgBAQAGPwLfhVtmIf/EABsQAAIBBQAAAAAAAAAAAAAAAAABESExUWGx/9oACAEBAAE/IYt60xxAgb4DCDqP/9oADAMBAAIAAwAAABBbD//EABYRAQEBAAAAAAAAAAAAAAAAAAABEf/aAAgBAwEBPxCSy//EABYRAQEBAAAAAAAAAAAAAAAAAAABEf/aAAgBAgEBPxC00//EAB0QAQADAAEFAAAAAAAAAAAAAAEAESGRMWFxgbH/2gAIAQEAAT8QSnoMRXyAJj9p5ljPcsMrOGUVBfmIpGif/9k=&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;Utah&quot;
        title=&quot;Utah&quot;
        src=&quot;/static/0021dbd702b4e8db3e12fd881f3daa86/70ebb/Utah.jpg&quot;
        srcset=&quot;/static/0021dbd702b4e8db3e12fd881f3daa86/f71f9/Utah.jpg 88w,
/static/0021dbd702b4e8db3e12fd881f3daa86/e52aa/Utah.jpg 175w,
/static/0021dbd702b4e8db3e12fd881f3daa86/70ebb/Utah.jpg 350w,
/static/0021dbd702b4e8db3e12fd881f3daa86/7349d/Utah.jpg 525w,
/static/0021dbd702b4e8db3e12fd881f3daa86/29d31/Utah.jpg 700w,
/static/0021dbd702b4e8db3e12fd881f3daa86/a0850/Utah.jpg 4096w&quot;
        sizes=&quot;(max-width: 350px) 100vw, 350px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;figcaption&gt;Driving across Utah. The landscape is surreal.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;figure&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 350px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/879c8bfd085be4b9909cd66deb13a814/a0850/LakeTahoe.jpg&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 75%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAPABQDASIAAhEBAxEB/8QAFgABAQEAAAAAAAAAAAAAAAAABAAB/8QAFgEBAQEAAAAAAAAAAAAAAAAAAAEC/9oADAMBAAIQAxAAAAFQyayuFR//xAAZEAACAwEAAAAAAAAAAAAAAAABAgADERL/2gAIAQEAAQUCDoYuJZbYAxnJ0z//xAAWEQEBAQAAAAAAAAAAAAAAAAAAERL/2gAIAQMBAT8Byj//xAAVEQEBAAAAAAAAAAAAAAAAAAAAEv/aAAgBAgEBPwG1v//EAB0QAAECBwAAAAAAAAAAAAAAAAABIRARIjEyQYH/2gAIAQEABj8CuZtLZS/RY//EABwQAAIDAAMBAAAAAAAAAAAAAAERACExQVFxof/aAAgBAQABPyENkewspuBfIm0LYAhOjsJMUcho2p//2gAMAwEAAgADAAAAEGsv/8QAFxEAAwEAAAAAAAAAAAAAAAAAAAERUf/aAAgBAwEBPxBUTh//xAAWEQEBAQAAAAAAAAAAAAAAAAABABH/2gAIAQIBAT8QcNpv/8QAGxABAQADAQEBAAAAAAAAAAAAAREAIUExUZH/2gAIAQEAAT8QA7DorzLx1Kwelg8yEQRw3+ZTAYEOk44K0O57kEgz5n//2Q==&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;LakeTahoe&quot;
        title=&quot;LakeTahoe&quot;
        src=&quot;/static/879c8bfd085be4b9909cd66deb13a814/70ebb/LakeTahoe.jpg&quot;
        srcset=&quot;/static/879c8bfd085be4b9909cd66deb13a814/f71f9/LakeTahoe.jpg 88w,
/static/879c8bfd085be4b9909cd66deb13a814/e52aa/LakeTahoe.jpg 175w,
/static/879c8bfd085be4b9909cd66deb13a814/70ebb/LakeTahoe.jpg 350w,
/static/879c8bfd085be4b9909cd66deb13a814/7349d/LakeTahoe.jpg 525w,
/static/879c8bfd085be4b9909cd66deb13a814/29d31/LakeTahoe.jpg 700w,
/static/879c8bfd085be4b9909cd66deb13a814/a0850/LakeTahoe.jpg 4096w&quot;
        sizes=&quot;(max-width: 350px) 100vw, 350px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;figcaption&gt;Views of South Lake Tahoe&lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;I also am no longer with the Deming Center Venture Fund as I resigned from my position as Portfolio Manager. I wanted to clear up commitments and focus on getting as much research accomplished this year as possible. It was a great experience and I wish the team the very best!&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Guest Post - Sunsetting Shoppy Bot]]></title><link>https://www.carl-mueller.com/posts/sunsetting-shoppy-bot/</link><guid isPermaLink="false">https://www.carl-mueller.com/posts/sunsetting-shoppy-bot/</guid><pubDate>Tue, 23 Jun 2020 22:00:00 GMT</pubDate><content:encoded>&lt;h3&gt;This is a post from my former partner in crime (Dylan La Com).&lt;/h3&gt;
&lt;p&gt;We started Lightning in a Bot Inc., together. Alas, ShoppyBot (and for the most part LIAB) has been set out to pasture. I share many of his sentiments below.&lt;/p&gt;
&lt;h3&gt;Sunsetting Shoppy Bot&lt;/h3&gt;
&lt;p&gt;Last Friday I made the decision to shut down Shoppy Bot, a software product I had been operating since 2016. It’s a decision I’d been contemplating vaguely for over a year, and more seriously within the last couple months. And while it’s sad to turn the lights off on a product I’ve poured so much effort into keeping the lights on, there is a certain peace that comes with accepting that it is time to move on.&lt;/p&gt;
&lt;p&gt;Ultimately, I came to understand that I was not going to be able to build Shoppy Bot into what I envisioned for it with the time and resources I have. And I can no longer continue supporting it in it’s current, free version.&lt;/p&gt;
&lt;p&gt;Here are a few of my favorite memories or highlights from Shoppy Bot:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Driving up to Carl’s house in Santa Barbara with Matt and Ian in May of 2016 to sign the incorporation paperwork. Talk about a Dream Team.&lt;/li&gt;
&lt;li&gt;Launching Shoppy Bot on the Shopify app store and getting featured on their homepage.&lt;/li&gt;
&lt;li&gt;Seeing Carl develop his machine learning skillset to create our Natural Language Processing technology.&lt;/li&gt;
&lt;li&gt;Meeting many, many amazing business owners, like Ashley running Pour This, Kenny and Nick running The Social Life, and David running Great Lakes. It was truly inspiring to be working with these and other incredible humans all doing really cool things.&lt;/li&gt;
&lt;li&gt;Of course, the many interesting technologies we worked on, including chat bots, language processing, reporting and analytics.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In many ways, Shoppy Bot is a major reason I am where I am today. It was an incredible period of growth and learning for me. And, from a career standpoint, it helped me leapfrog into job positions and opportunities that I don’t think I would have otherwise gotten.&lt;/p&gt;
&lt;p&gt;So what’s next for me?&lt;/p&gt;
&lt;p&gt;I want to get back in touch with my sense of curiousity, and let myself go down some rabbit holes. One skill I’d like to improve is my writing, hence I’m dusting off this website you’re reading right now. I also want to spend time thinking about other projects. Some old––things I’ve wanted to do for a while––and some new. And I’m starting with one very specific question…&lt;/p&gt;
&lt;p&gt;What are the most important problems in my field, and why am I not working on them?&lt;/p&gt;
&lt;p&gt;This question can be traced back to Bell Labs and a &lt;a href=&quot;http://www.cs.virginia.edu/~robins/YouAndYourResearch.html&quot;&gt;speech by Turing Award winner Richard Hamming&lt;/a&gt; given in 1986. It comes up ever so often in the writing of other inspiring figures, including Paul Graham in his 2005 essay &lt;a href=&quot;http://www.paulgraham.com/procrastination.html&quot;&gt;“Good and Bad Procrastination”&lt;/a&gt;. More recently, Sam Altman of Y Combinator and OpenAI &lt;a href=&quot;https://blog.samaltman.com/researchers-and-founders&quot;&gt;wrote&lt;/a&gt;, “In general, no one reflects on this question enough, but the best people do it the most”.&lt;/p&gt;
&lt;p&gt;One thing I’ve learned about myself is I’m drawn to the “mission”. I want to apply my energy to big hairy problems that have the potential to help hundreds, thousands, perhaps millions of people. There’s still so much left to do.&lt;/p&gt;
&lt;p&gt;I want to thank all the amazing people that have worked on Shoppy Bot or helped in one way or another over the years. Whether it was a simple conversation or an introduction you made, it means the world to me, thank you! If &lt;a href=&quot;https://www.youtube.com/watch?v=rBpaUICxEhk&quot;&gt;life is like a dance&lt;/a&gt;, you joined my dance, if only momentarily, and gave it a little twist, a zig, a zag, but forever left your mark on an immutable composition.&lt;/p&gt;
&lt;p&gt;And so with that, farewell Shoppy Bot, and thanks for the good times.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[IROS, Departmental Award & Other Updates]]></title><link>https://www.carl-mueller.com/posts/update-2018/</link><guid isPermaLink="false">https://www.carl-mueller.com/posts/update-2018/</guid><pubDate>Wed, 23 Jan 2019 22:00:00 GMT</pubDate><content:encoded>&lt;p&gt;This past October, I got to travel to Madrid Spain to present my research at IROS 2018. It was a wonderful experience and I was so grateful for the opportunity. As my first trip to Europe, I was quite enamored with Madrid. Although chances are any European city would have charmed me, others told me Madrid is definitely a standout in Europe. I can’t wait to make it back which is more motivation to publish my next paper!&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 350px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/dc49688db46d3d751779986c28df9dab/a0850/madridpalace.jpg&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 75%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAPABQDASIAAhEBAxEB/8QAGAAAAwEBAAAAAAAAAAAAAAAAAAECAwT/xAAWAQEBAQAAAAAAAAAAAAAAAAAAAQL/2gAMAwEAAhADEAAAAeCpWazMP//EABsQAAMAAgMAAAAAAAAAAAAAAAABAgMEERIU/9oACAEBAAEFAo18lS9ejzoV8Do7H//EABURAQEAAAAAAAAAAAAAAAAAAAAR/9oACAEDAQE/AUf/xAAVEQEBAAAAAAAAAAAAAAAAAAAAEf/aAAgBAgEBPwGq/8QAHhAAAAQHAAAAAAAAAAAAAAAAAAECIQMQETFBYZH/2gAIAQEABj8Cqmx7GOh4iQxz/8QAHBAAAgICAwAAAAAAAAAAAAAAAAERMWFxIUFR/9oACAEBAAE/IV9SsCTahsiCg8HcFrgltzsl4f/aAAwDAQACAAMAAAAQt+//xAAWEQEBAQAAAAAAAAAAAAAAAAAAEQH/2gAIAQMBAT8Qmqf/xAAVEQEBAAAAAAAAAAAAAAAAAAAAEf/aAAgBAgEBPxBt/8QAGxABAAIDAQEAAAAAAAAAAAAAAQARITFBgWH/2gAIAQEAAT8QXxLiIfSXt2sZccToTKvY4QV2YmClReo0FAOUT//Z&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;madridpalace&quot;
        title=&quot;madridpalace&quot;
        src=&quot;/static/dc49688db46d3d751779986c28df9dab/70ebb/madridpalace.jpg&quot;
        srcset=&quot;/static/dc49688db46d3d751779986c28df9dab/f71f9/madridpalace.jpg 88w,
/static/dc49688db46d3d751779986c28df9dab/e52aa/madridpalace.jpg 175w,
/static/dc49688db46d3d751779986c28df9dab/70ebb/madridpalace.jpg 350w,
/static/dc49688db46d3d751779986c28df9dab/7349d/madridpalace.jpg 525w,
/static/dc49688db46d3d751779986c28df9dab/29d31/madridpalace.jpg 700w,
/static/dc49688db46d3d751779986c28df9dab/a0850/madridpalace.jpg 4096w&quot;
        sizes=&quot;(max-width: 350px) 100vw, 350px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;figcaption&gt;Palacio Real de Madrid.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 350px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/882c6ffdca5baff623ac0480565ae92f/67226/CurvyMadrid.jpg&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 132.95454545454547%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAbABQDASIAAhEBAxEB/8QAGQAAAgMBAAAAAAAAAAAAAAAAAAIBBAUD/8QAFgEBAQEAAAAAAAAAAAAAAAAAAgED/9oADAMBAAIQAxAAAAGNTBsFUxCLo0S80GJf/8QAHBAAAQQDAQAAAAAAAAAAAAAAAQACAxEEIkEj/9oACAEBAAEFAoCBLknzdqQSHyyl7aKu0VsucX//xAAYEQACAwAAAAAAAAAAAAAAAAAAARAREv/aAAgBAwEBPwEVxs//xAAXEQEAAwAAAAAAAAAAAAAAAAAAEBIh/9oACAECAQE/AWRV/8QAGRAAAwADAAAAAAAAAAAAAAAAABARAQIx/9oACAEBAAY/AlFqdLXl/wD/xAAeEAEAAgEEAwAAAAAAAAAAAAABABEhEDFBUWGB4f/aAAgBAQABPyHpSpABvlxKwI0aIY70cR+yBxelQbBxDwuUFB1OGn//2gAMAwEAAgADAAAAEOA/gv/EABcRAAMBAAAAAAAAAAAAAAAAAAABIRH/2gAIAQMBAT8QykYaxMj/xAAWEQEBAQAAAAAAAAAAAAAAAAAAARH/2gAIAQIBAT8QimRh/8QAHRABAAMBAAIDAAAAAAAAAAAAAQARITFBUXGBof/aAAgBAQABPxC4XaLeLXPy4Y6dRzjGEeD32KLFBotcqYxCAYeB9x6Xaij0IVpMvBPmFS6Bl6Qa43jnJcQOc9BOs9jP/9k=&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;CurvyMadrid&quot;
        title=&quot;CurvyMadrid&quot;
        src=&quot;/static/882c6ffdca5baff623ac0480565ae92f/70ebb/CurvyMadrid.jpg&quot;
        srcset=&quot;/static/882c6ffdca5baff623ac0480565ae92f/f71f9/CurvyMadrid.jpg 88w,
/static/882c6ffdca5baff623ac0480565ae92f/e52aa/CurvyMadrid.jpg 175w,
/static/882c6ffdca5baff623ac0480565ae92f/70ebb/CurvyMadrid.jpg 350w,
/static/882c6ffdca5baff623ac0480565ae92f/7349d/CurvyMadrid.jpg 525w,
/static/882c6ffdca5baff623ac0480565ae92f/29d31/CurvyMadrid.jpg 700w,
/static/882c6ffdca5baff623ac0480565ae92f/67226/CurvyMadrid.jpg 3072w&quot;
        sizes=&quot;(max-width: 350px) 100vw, 350px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;figcaption&gt;Madrid is a beautiful city.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 350px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/98a4c09673725c640714846264bd6677/a0850/elmatador.jpg&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 75%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAPABQDASIAAhEBAxEB/8QAFgABAQEAAAAAAAAAAAAAAAAABAAC/8QAFQEBAQAAAAAAAAAAAAAAAAAAAQL/2gAMAwEAAhADEAAAASMHiKdCh//EABkQAAIDAQAAAAAAAAAAAAAAAAEDAAISIv/aAAgBAQABBQKt4loEY4aIJm+rOwf/xAAYEQACAwAAAAAAAAAAAAAAAAAAAQIhUf/aAAgBAwEBPwFR0o//xAAXEQEAAwAAAAAAAAAAAAAAAAAAEiFR/9oACAECAQE/AZYt/8QAHhAAAgEDBQAAAAAAAAAAAAAAAAERAhIhEyIxQZH/2gAIAQEABj8Ce6IHd2+TCMEOhEadHh//xAAbEAEAAwADAQAAAAAAAAAAAAABABEhMUFRkf/aAAgBAQABPyFD4lpASrLNXOcEqUOTKnfqekeVf//aAAwDAQACAAMAAAAQq8//xAAXEQADAQAAAAAAAAAAAAAAAAAAESEB/9oACAEDAQE/EM0IjP/EABcRAAMBAAAAAAAAAAAAAAAAAAABESH/2gAIAQIBAT8QdMa2H//EAB0QAQEAAgIDAQAAAAAAAAAAAAERACFBUTFhgcH/2gAIAQEAAT8QA2WoLO+MQpAN6Q5wBCA2nnPViAybtp+3FDdUlp3EyrpARE0+5//Z&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;elmatador&quot;
        title=&quot;elmatador&quot;
        src=&quot;/static/98a4c09673725c640714846264bd6677/70ebb/elmatador.jpg&quot;
        srcset=&quot;/static/98a4c09673725c640714846264bd6677/f71f9/elmatador.jpg 88w,
/static/98a4c09673725c640714846264bd6677/e52aa/elmatador.jpg 175w,
/static/98a4c09673725c640714846264bd6677/70ebb/elmatador.jpg 350w,
/static/98a4c09673725c640714846264bd6677/7349d/elmatador.jpg 525w,
/static/98a4c09673725c640714846264bd6677/29d31/elmatador.jpg 700w,
/static/98a4c09673725c640714846264bd6677/a0850/elmatador.jpg 4096w&quot;
        sizes=&quot;(max-width: 350px) 100vw, 350px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;figcaption&gt;El Matador Bar - My go-to.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;While I’m amidst a “sophomore slump” with regards to my next publication, I have one major project in the pipeline as well as a chance to be second author on two other publications related to my own research. Two colleagues are taking my research and expanding upon it through the lens of augmented reality and natural language. These projects are likely to target the coming fall deadlines. Fall may seem far away, but time sure does go by quickly these days. &lt;/p&gt;
&lt;p&gt;This past spring break, I got to travel back to my “home away from home” (Santa Barbara) to see friends and be with Missy. It was so nice to spend time together as we’re still in a long distance relationship. We had a wonderful time and got to see the California “superbloom” and spend a few days in snowy Mammoth Lakes snowboarding.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 350px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/003d0b853f32d365f9204c26efa1f7ec/67226/mammothdinner.jpg&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 132.95454545454547%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAbABQDASIAAhEBAxEB/8QAGQAAAgMBAAAAAAAAAAAAAAAAAAQCAwUB/8QAFQEBAQAAAAAAAAAAAAAAAAAAAgP/2gAMAwEAAhADEAAAAc2c7gkzgi1O7PnR0XJr/8QAHhAAAQMEAwAAAAAAAAAAAAAAAgEDEQASEyIhIzH/2gAIAQEAAQUCHVyRyXjIQtLyhNlLbSnRt2U0ujnUOQjVPP/EABcRAAMBAAAAAAAAAAAAAAAAAAABESH/2gAIAQMBAT8BhBPSo//EABcRAQEBAQAAAAAAAAAAAAAAAAAREkH/2gAIAQIBAT8BVxmv/8QAHhAAAgICAgMAAAAAAAAAAAAAAAECESExEiIyQVH/2gAIAQEABj8CvBco8h9KJasUfZ1TLTr6J3vR5FwwXJ2I/8QAHBAAAgIDAQEAAAAAAAAAAAAAAAERITFBUXFh/9oACAEBAAE/IZMSHGQsjXCcFvQ3KU4KSEhkp2gxZLfoxaDO+BsnBSCHZLO3jt2VL4U8j//aAAwDAQACAAMAAAAQ/Mdy/8QAGBEBAQEBAQAAAAAAAAAAAAAAAQAhEXH/2gAIAQMBAT8QQjl5u2CSdL//xAAYEQEBAQEBAAAAAAAAAAAAAAABEQAhQf/aAAgBAgEBPxAUiul7cj6y1bv/xAAdEAEAAgMBAQEBAAAAAAAAAAABESEAMUFhUXGh/9oACAEBAAE/EL0qOuR7A9yG2QsUsbr2JzScGamCteYpWISSVU/hhIO7WiEv9jma7AkM+sV8VhAyWMZQt6zzO8aKs7wz2tMWJ8RpxgjhAB8ArEgKIZ//2Q==&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;mammothdinner&quot;
        title=&quot;mammothdinner&quot;
        src=&quot;/static/003d0b853f32d365f9204c26efa1f7ec/70ebb/mammothdinner.jpg&quot;
        srcset=&quot;/static/003d0b853f32d365f9204c26efa1f7ec/f71f9/mammothdinner.jpg 88w,
/static/003d0b853f32d365f9204c26efa1f7ec/e52aa/mammothdinner.jpg 175w,
/static/003d0b853f32d365f9204c26efa1f7ec/70ebb/mammothdinner.jpg 350w,
/static/003d0b853f32d365f9204c26efa1f7ec/7349d/mammothdinner.jpg 525w,
/static/003d0b853f32d365f9204c26efa1f7ec/29d31/mammothdinner.jpg 700w,
/static/003d0b853f32d365f9204c26efa1f7ec/67226/mammothdinner.jpg 3072w&quot;
        sizes=&quot;(max-width: 350px) 100vw, 350px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;figcaption&gt;Dinner in Mammoth with Missy.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 350px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/8b5393a15490e5a9faaa121dd7224e25/a0850/missyflowers.jpg&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 75%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAPABQDASIAAhEBAxEB/8QAFwAAAwEAAAAAAAAAAAAAAAAAAAEEA//EABYBAQEBAAAAAAAAAAAAAAAAAAEAA//aAAwDAQACEAMQAAAB2cizaSIL/8QAHBAAAQMFAAAAAAAAAAAAAAAAAQACEQMQEhMh/9oACAEBAAEFAhUlF8LaFgXs7b//xAAXEQADAQAAAAAAAAAAAAAAAAAAARES/9oACAEDAQE/AcoiP//EABgRAAIDAAAAAAAAAAAAAAAAAAASAQIR/9oACAECAQE/AXnBrH//xAAbEAABBAMAAAAAAAAAAAAAAAAAAQIQESExMv/aAAgBAQAGPwLDTk0LU//EABsQAQACAgMAAAAAAAAAAAAAAAEAESFRQWHw/9oACAEBAAE/IQxse2Cqas8WFr1uOzccPM//2gAMAwEAAgADAAAAEAQP/8QAFhEBAQEAAAAAAAAAAAAAAAAAAGGx/9oACAEDAQE/EII6/8QAGREAAwADAAAAAAAAAAAAAAAAAAEhEWGh/9oACAECAQE/EHVmm/iP/8QAGxABAQADAAMAAAAAAAAAAAAAAREAITFhcZH/2gAIAQEAAT8QsaTQ0Y+EHa4u84WgBLspLPmEJ2nXXjFTQ9Jn/9k=&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;missyflowers&quot;
        title=&quot;missyflowers&quot;
        src=&quot;/static/8b5393a15490e5a9faaa121dd7224e25/70ebb/missyflowers.jpg&quot;
        srcset=&quot;/static/8b5393a15490e5a9faaa121dd7224e25/f71f9/missyflowers.jpg 88w,
/static/8b5393a15490e5a9faaa121dd7224e25/e52aa/missyflowers.jpg 175w,
/static/8b5393a15490e5a9faaa121dd7224e25/70ebb/missyflowers.jpg 350w,
/static/8b5393a15490e5a9faaa121dd7224e25/7349d/missyflowers.jpg 525w,
/static/8b5393a15490e5a9faaa121dd7224e25/29d31/missyflowers.jpg 700w,
/static/8b5393a15490e5a9faaa121dd7224e25/a0850/missyflowers.jpg 4096w&quot;
        sizes=&quot;(max-width: 350px) 100vw, 350px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;figcaption&gt;Missy enoying the wildflower bloom in California.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;More recently, this week I was awarded the Departmental Award for Outstanding PhD Research and was accepted to the Deming Center Venture Fund (DCVF). The award recognizes four computer science PhD students for novel and impactful research within their field. The DCVF is a student run venture capital fund that invests in newly formed startups in the local Denver area.  Exciting stuff!&lt;/p&gt;
&lt;p&gt;Spring is on its way and biking season is upon us! Whoop whoop!&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Concept Constrained Learning from Demonstration]]></title><link>https://www.carl-mueller.com/research/cc-lfd/</link><guid isPermaLink="false">https://www.carl-mueller.com/research/cc-lfd/</guid><pubDate>Fri, 26 Oct 2018 00:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;Why is this important?&lt;/h2&gt;
&lt;p&gt;My PhD research’s seminal work is called &lt;strong&gt;Concept Constrained Learning from Demonstration&lt;/strong&gt; &lt;span id=&quot;citation-0&quot; data-hover=&quot;&quot;&gt;&lt;span class=&quot;citation-number&quot;&gt;[1]&lt;/span&gt;&lt;/span&gt;. It is this work that motivates my current research into constrained motion planning and human-robot interfaces, specifically augmented reality interfaces. Ultimately, I want to figure out how to make robots easily trainable and usable by non-roboticists. Physical automation is a wide-open frontier in the world of information technology. However, the introduction of collaborative robots into human environments presents a number of challenges often not required of large-scale industrial robots: safety in shared workspaces, rapidly changing task requirements, decision-making, and, perhaps most challenging, adhering to human expectations of behavior. As such, the foundational motivation behind this work is to provide human users the means to easily train collaborative robots to execute dynamic skills while adhering to important behavioral restrictions.&lt;/p&gt;
&lt;h2&gt;Background&lt;/h2&gt;
&lt;h3&gt;Learning from Demonstration&lt;/h3&gt;
&lt;p&gt;Robot Learning from Demonstration (LfD) consists of methods that attempt to learn successful robot behavior models from human input. A human operator interacts with a robotic system through some mode of demonstration, usually through kinesthetic demonstration, teleoperation, or passive observation &lt;sup id=&quot;fnref-1&quot;&gt;&lt;a href=&quot;#fn-1&quot; class=&quot;footnote-ref&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;. Demonstration ideally communicates information about the nature of the skill that the robotic learning system uses to build a learned model that resembles some latent (i.e. hidden) ground truth model. The methods by which robotic systems learn such models spans across the spectrum of machine learning. However there are three broad categorizations for robot LfD systems: 1) plan learning, 2) functional optimization, and 3) policy learning  &lt;span id=&quot;citation-0&quot; data-hover=&quot;&quot;&gt;&lt;span class=&quot;citation-number&quot;&gt;[2]&lt;/span&gt;&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 350px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/4d3b3c608170205bce549ef23656749a/3acf0/ActionShot.jpg&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 55.68181818181818%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAALABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAAAAMCBP/EABYBAQEBAAAAAAAAAAAAAAAAAAABAv/aAAwDAQACEAMQAAABtKW805h//8QAHBAAAgEFAQAAAAAAAAAAAAAAAgMTAAEREiIy/9oACAEBAAEFAm9U8hiWBajfLHAMI+f/xAAWEQEBAQAAAAAAAAAAAAAAAAAAARH/2gAIAQMBAT8Bka//xAAWEQADAAAAAAAAAAAAAAAAAAAAARH/2gAIAQIBAT8BbhD/xAAaEAACAwEBAAAAAAAAAAAAAAAAAQIRIRKh/9oACAEBAAY/AuBQXpsRpl1tCP/EABwQAQACAgMBAAAAAAAAAAAAAAEAESExQVGBsf/aAAgBAQABPyF0e0cvqvEZZ8uKOIzywv5GW6GY3cmJ/9oADAMBAAIAAwAAABBrH//EABcRAAMBAAAAAAAAAAAAAAAAAAABEVH/2gAIAQMBAT8QoqRh/8QAFxEAAwEAAAAAAAAAAAAAAAAAAAERQf/aAAgBAgEBPxC6Dd6f/8QAGxABAAMBAAMAAAAAAAAAAAAAAQARIVExQXH/2gAIAQEAAT8QoIrI6/dm3J2eB18seg2Oo4qsj4hVOagPkRT1DGABg1P/2Q==&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;ActionShot&quot;
        title=&quot;ActionShot&quot;
        src=&quot;/static/4d3b3c608170205bce549ef23656749a/70ebb/ActionShot.jpg&quot;
        srcset=&quot;/static/4d3b3c608170205bce549ef23656749a/f71f9/ActionShot.jpg 88w,
/static/4d3b3c608170205bce549ef23656749a/e52aa/ActionShot.jpg 175w,
/static/4d3b3c608170205bce549ef23656749a/70ebb/ActionShot.jpg 350w,
/static/4d3b3c608170205bce549ef23656749a/7349d/ActionShot.jpg 525w,
/static/4d3b3c608170205bce549ef23656749a/29d31/ActionShot.jpg 700w,
/static/4d3b3c608170205bce549ef23656749a/3acf0/ActionShot.jpg 2000w&quot;
        sizes=&quot;(max-width: 350px) 100vw, 350px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;figcaption&gt;A human user (me) kinestheticaly demonstrating a skill.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;The ultimate goal of these learning methods is to facilitate the transfer of information from a non-roboticist, with some expert intuition about the skill, to the robotic learning system. This information is then used by robot skill learning methods to produce successful learned models of the task. Plan learning methods attempt to learn models that operate at high levels of task abstraction, either learning a primitive sequence or hierarchy. Functional optimization methods either directly optimize a candidate trajectory (potentially one derived from demonstration) using a known objective function, or they attempt to learn an objective from demonstration. These approaches often emulate or directly draw from Reinforcement Learning and Inverse Reinforcement Learning techniques. Lastly, policy learning methods produce models that output either trajectories or low-level actions directly &lt;sup id=&quot;fnref-2&quot;&gt;&lt;a href=&quot;#fn-2&quot; class=&quot;footnote-ref&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;h3&gt;Keyframe Learning from Demonstration&lt;/h3&gt;
&lt;p&gt;CC-LfD is an augmentation of a learning method called Keyframe LfD (KLfD) &lt;span id=&quot;citation-0&quot; data-hover=&quot;&quot;&gt;&lt;span class=&quot;citation-number&quot;&gt;[3]&lt;/span&gt;&lt;/span&gt;. In traditional KLfD, human operators teach a skill by providing distinct waypoints of robot state data. This represents a coarse trajectory for the robot to follow. This approach is powerful because it very easily allows users to specify robot motion, but it is somewhat brittle as the learned skill is really a concrete instantiation of one robot trajectory. Any variation to the environment or to changes in user expectations cannot be accommodated.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 350px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/c0c8ce24e8bfeb84899aa7e53e331d58/064b2/Akgun_Keyframe_LfD.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 32.95454545454546%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAHCAIAAACHqfpvAAAACXBIWXMAAAsSAAALEgHS3X78AAAA8ElEQVQY0zWR2W6FMAxE8///WGhBBSEC2cnCdk8T1Q/WeLyM4whTTSnlnDvPM4RgrfXeA85SUorTKLdVLUbHnK/r2vd9miYpJWWC4L7vnDNxSolBVMDQH2P03mkd3vf9ldL4UHKmnuLjOOZ5FsuyaK2hkIIFM6LhUsq+k7LOR6ksYetBc9s2pot1XWGRep4HKRJ3NdTg2f6I6apZNoJhNaa3UCAFxWvxqNEMuP5Dxq/Wk2WPlP+M66DByziHaEsabdqercE6Bw/GU4oAvhUA6LfW0SW+q43j+FOt73v8MIxd1/XVSFU/ALrui1O3v+B3Pj2/jm6z6+44AAAAAElFTkSuQmCC&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;Akgun Keyframe LfD&quot;
        title=&quot;Akgun Keyframe LfD&quot;
        src=&quot;/static/c0c8ce24e8bfeb84899aa7e53e331d58/13ae7/Akgun_Keyframe_LfD.png&quot;
        srcset=&quot;/static/c0c8ce24e8bfeb84899aa7e53e331d58/942f4/Akgun_Keyframe_LfD.png 88w,
/static/c0c8ce24e8bfeb84899aa7e53e331d58/4edbd/Akgun_Keyframe_LfD.png 175w,
/static/c0c8ce24e8bfeb84899aa7e53e331d58/13ae7/Akgun_Keyframe_LfD.png 350w,
/static/c0c8ce24e8bfeb84899aa7e53e331d58/52211/Akgun_Keyframe_LfD.png 525w,
/static/c0c8ce24e8bfeb84899aa7e53e331d58/8c557/Akgun_Keyframe_LfD.png 700w,
/static/c0c8ce24e8bfeb84899aa7e53e331d58/064b2/Akgun_Keyframe_LfD.png 1874w&quot;
        sizes=&quot;(max-width: 350px) 100vw, 350px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;figcaption&gt;A visual representation of keyframe learning from demonstration (Akgun, 2012) where trajectories that outline a ‘P’ are clustering into keyframes.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;Keyframe LfD can be made more robust through automating keyframe generation and through statistical learning. To automate this approach, users first provide high-rate-of-sampling demonstration trajectories of the skill, ideally expressing subtle variation. Demonstration trajectories are aligned using a technique called Dynamic Time Warping &lt;span id=&quot;citation-0&quot; data-hover=&quot;&quot;&gt;&lt;span class=&quot;citation-number&quot;&gt;[?]&lt;/span&gt;&lt;/span&gt;, which is an algorithmic method to align similar regions in sequential data from one sequence to another. The data points of these temporally aligned demonstration trajectories are clustered into sequential groups across demonstrations. These clusters of robot state data are fitted to learned &lt;em&gt;keyframe distributions&lt;/em&gt;, which are used to generate waypoints that the robot follows sequentially to perform a skill. Forming statistical distributions to represent keyframes, as opposed to single points, enables the LfD algorithm to adapt to behavioral restrictions the human operator might decide to place on the robot.&lt;/p&gt;
&lt;h2&gt;Concept Constrained Learning from Demonstration&lt;/h2&gt;
&lt;p&gt;To this end, an algorithm called Concept Constrained Learning from Demonstration integrates behavioral restrictions, communicated by the user, into the keyframe LfD model. These restrictions are dubbed ‘concept constraints’ which represent prohibitions on the behavior of the robot system. This is accomplished by encoding concept constraints as Boolean planning predicates which when combined into logical forumlae and represent a multitude of concurrent constraints on the learning of keyframes. These planning predicates serve as rejection samplers during the distribution learning phase as well as the skill reconstruction phase. &lt;/p&gt;
&lt;h3&gt;Algorithm Overview&lt;/h3&gt;
&lt;p&gt;The algorithm follows these broad steps (right to left in the image cluster): &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Recording of Demonstration Trajectories and Constraints&lt;/li&gt;
&lt;li&gt;Alignment of Trajectories&lt;/li&gt;
&lt;li&gt;Clustering and Rejection Sampling Models&lt;/li&gt;
&lt;li&gt;Culling of Keyframes&lt;/li&gt;
&lt;li&gt;Remodeling&lt;/li&gt;
&lt;li&gt;Skill Execution&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;Recording of Demonstration Trajectories&lt;/h4&gt;
&lt;p&gt;The first step in CC-LfD, as with most LfD algorithms, is to capture demonstration trajectories provided by human operators. In CC-LfD, robot demonstration trajectories are captured via kinesthetic demonstration. What is unique to the CC-LfD demonstration process is that human teachers can provide constraint annotations. An annotation indicates the spatio-temporal region of a trajectory where a given constraint must hold true. In the published version of this work &lt;span id=&quot;citation-0&quot; data-hover=&quot;&quot;&gt;&lt;span class=&quot;citation-number&quot;&gt;[1]&lt;/span&gt;&lt;/span&gt;, this constraint annotation was done manually through programmed arm cuff buttons on the robot during experimentation. However, future work plans to use an augmented reality interface or to utilize a natural language interface. &lt;/p&gt;
&lt;p&gt;&lt;figure&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 350px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/0044ff6f28d0645e64f8278a05838d70/a58fe/Demonstration.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 97.72727272727272%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAIAAAAC64paAAAACXBIWXMAAAsSAAALEgHS3X78AAACfElEQVQ4y31T2W7aQBT1l/YD+gH9AB5oK+UZ8UCR4KVSGgqUlpQS1rDv+1ow2CxR2AwUQ8A2PZ5JDKlQj2Rr5s7dzpk7zHA43G63x+Nxs9lUq9VerzcajYYEWPT7/R4By/YGPDd5GFILy7LNZpOJRqOz2QzBq9UK1k6nU6vVcrlcsVjMZrOTyQRHiqLgPxPWwRInSdKRAFmYZDK5WCywWS6XtBpL8JtAEAQcSbKseo8eP7lzori9EIzKHMehICyJRMLn8wWDQWTBkUyC1fqKTHGh8ng81pr8D6jDq+D1eg3OqVQqHo+DbYIgQtBut+EAFuFw2Gq1Qips4awGz+dzWnkwGIDkdDqFTo8EDwTIC53K5TJqmkwmj8cD/263e6r89PSE/YCAKkeBLbSgV8jzPFIjHbSA/RSsENDF/gXaxVDZcAQLFQx4FUybAVVwuycAYdx5qVSCllpqTbZTMJBOpxFJ75bicDigSfQMCf1+P+icx58EazQaiLx4VdQiimIgEKhUKpqFoaXAxOv1guR5DOH4PBZaxlAoRCdHbRu3ipSYZ7wKmKYEuBtxuz0vqwmGAhg+0HluG96xaGS1XNKRyGdS198DHz7Hspn0QhA0bbVE9XodEqrBELNaKf8KRt13IZ7rU6c/4q7VH0MnKvhutztvAU8Yzatt5/MF788fV9bbbzEMnSKdMaR/juPvQwFF2r/UVo14yKDGILFOp2u1WurTk2isLBFgsSfcvtxlnJHaUTlIMj51bAqFApRmjEajwWDQ3t0/kEmddJV98/bdTtzQAvg7HA69Xs+43W6z2exyuWw229dLsNsddtv11cf3NzeqA9ycTqfFYgHtv1SdNn/PFKnjAAAAAElFTkSuQmCC&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;Demonstration&quot;
        title=&quot;Demonstration&quot;
        src=&quot;/static/0044ff6f28d0645e64f8278a05838d70/13ae7/Demonstration.png&quot;
        srcset=&quot;/static/0044ff6f28d0645e64f8278a05838d70/942f4/Demonstration.png 88w,
/static/0044ff6f28d0645e64f8278a05838d70/4edbd/Demonstration.png 175w,
/static/0044ff6f28d0645e64f8278a05838d70/13ae7/Demonstration.png 350w,
/static/0044ff6f28d0645e64f8278a05838d70/52211/Demonstration.png 525w,
/static/0044ff6f28d0645e64f8278a05838d70/a58fe/Demonstration.png 548w&quot;
        sizes=&quot;(max-width: 350px) 100vw, 350px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;figcaption&gt;Recording and constraint annotation&lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;h4&gt;Alignment&lt;/h4&gt;
&lt;p&gt;After capturing demonstration trajectories, an important step is to generate a mapping between the points of one trajectory with points in the other. Under the assumption that a human operator demonstrates repeatedly the same skill, drawing from a consistent skill distribution, it is important to be able to know which regions of one trajectory correspond with regions in another.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 350px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/230a2bc27b98ebc2de4172a8486c38c4/432e7/Alignment.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 96.59090909090908%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAATCAIAAAAf7rriAAAACXBIWXMAAAsSAAALEgHS3X78AAACgUlEQVQ4y31UyXLaQBDlr+M/4ANcRYWqXInPPsVVgFjMbmF2UxSrJBMWIdAGGmkkyBtNTGTHSR9GrZ5+09OvnxRbr9eEkEvEFovFfD7f7XaqqmqatlqtZFlWFOWVmfKqKMvlcjKZHI/HWLVa3e/3wJzPZw5uNBq1Wq3ZbIqimMvler0egkHguy4lLnWIFwQXnHg4HGJPT094RMGdTqfVao1Go263WyqV4KAI4gfLMWxysIjl+NvtVtf1GOp8AD8/P1cqFcTzoWUymUKhcN3lzudgrOgZG6gGLvzQooz8DwxDI+g5nU7X6/Vo3Dk5J/tom1bgB+DSMIxPwGAItGIbg0D905tZpmUapqEbxCGSLDHCrmCOh2FI4F8PDddbhYbxsMc6XNZrDA9NvavMizuO8zM05BHiXP5t7ypDEjkhhwhGhXXQ75fKVVBdLpen06lHvXDgwbXMH7Bt2w8/HpbKK3xKqeuyVFW34WALBwlZAReOssPAXGE4XpFliAj+ZrOh1DOPkAS7NnID6pu6KQgCmr/iGRgHg5h0Jo3QKRQTV7umnzxK2atDDrs9j2OEyOf4WL1Rty1bFJuqup1Np8V8oVyptNvt1UaFGJEEkexV7Qxx+z64HI/HEMJvMMQITLPVromdcqnkeaxDltFs5/LFXre726hIRRwj5Dd6fHzE2Nm18RmAz/v7+/7gJUoG96WFlM1kQYFlWbwy2IbyZrMZA6NyIpHAYZxkLmb65iEIhvBtoaam7TEX4roQwGAwYOC7u7t4PP6h5tVQB+vX5LdiRYR3JNQLLnN5dXPzBd3FUqkU7gy5DYfD0V+GoCRJt7e3qdR3/EyGw5fJeAwJJZNJnPsLjzsPnBQ0NIYAAAAASUVORK5CYII=&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;Alignment&quot;
        title=&quot;Alignment&quot;
        src=&quot;/static/230a2bc27b98ebc2de4172a8486c38c4/13ae7/Alignment.png&quot;
        srcset=&quot;/static/230a2bc27b98ebc2de4172a8486c38c4/942f4/Alignment.png 88w,
/static/230a2bc27b98ebc2de4172a8486c38c4/4edbd/Alignment.png 175w,
/static/230a2bc27b98ebc2de4172a8486c38c4/13ae7/Alignment.png 350w,
/static/230a2bc27b98ebc2de4172a8486c38c4/52211/Alignment.png 525w,
/static/230a2bc27b98ebc2de4172a8486c38c4/432e7/Alignment.png 570w&quot;
        sizes=&quot;(max-width: 350px) 100vw, 350px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;figcaption&gt;Trajectory alignment using DTW.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;One important caveat in this alignment process is that the algorithm takes special note of constraint annotation boundaries, regions in spatio-temporal regions in a trajectory where the set of applied constraints changes.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 350px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/1ff9e36847d86ddf6447e406e0194c18/e40ed/DTW.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 50%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAYAAAC0VX7mAAAACXBIWXMAAAsSAAALEgHS3X78AAABg0lEQVQoz61SS07DMBDN0bgEa9ZsS4OQEFwBFpCABIg7INpu2HEDJFAlAkGgNknT1k7iz3jshEk/YgEskBjpad7z2JbHb7yqqg6stQkiPhKGP4HqQ7vOa76uLfFkLb4ZYwae1jpo/hL17yXnXOQppY5XWtR1DZSBzixyC2cM6KIE1AA4ycFWFUhJQrUAgCVXFrBx1j60L7xu/inoQROPcbYhs2y/HCcdlmYdNk66PHrxRZruiDjenUevfvUc7ako8mWS7Ehap/1LpGm31SqOfTUa7ZEfWx4vik1TFNdmOj0xnJ+aaR5iPgnLLA/K949zOx6dyfjtHObz0DAWIOcBzmYBFkVg8jwE4pBlATB2oaU89OgHrv6tZefS9g+PFkII0UgJNaHRGsycgUMEGpOFSY4MA2NBA4IgM6QyoCgrEkimNLgyBbQ+ay80dE9j3cr/5ov/Iax1scc536a274WQN4yXfSHVQArZI93XWg1oWHvURZ/G6xunoem1exDNLem7SojLT6CB7nloDitRAAAAAElFTkSuQmCC&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;DTW&quot;
        title=&quot;DTW&quot;
        src=&quot;/static/1ff9e36847d86ddf6447e406e0194c18/13ae7/DTW.png&quot;
        srcset=&quot;/static/1ff9e36847d86ddf6447e406e0194c18/942f4/DTW.png 88w,
/static/1ff9e36847d86ddf6447e406e0194c18/4edbd/DTW.png 175w,
/static/1ff9e36847d86ddf6447e406e0194c18/13ae7/DTW.png 350w,
/static/1ff9e36847d86ddf6447e406e0194c18/52211/DTW.png 525w,
/static/1ff9e36847d86ddf6447e406e0194c18/8c557/DTW.png 700w,
/static/1ff9e36847d86ddf6447e406e0194c18/e40ed/DTW.png 1378w&quot;
        sizes=&quot;(max-width: 350px) 100vw, 350px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;figcaption&gt;An example of an alignment of two sequences using Dynamic Time Warping.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;h4&gt;Clustering and Rejection Sampling Modeling&lt;/h4&gt;
&lt;p&gt;The bulk of the algorithm’s novelty lies in the way it facilities the learning of keyframes. Once an alignment is completed, sequential clusters of aligned points serve as the basis for keyframe models. Each cluster of points are fitted with a statistical distribution (in this case Kernel Density Estimation). By modeling waypoints as distributions, rather than single points, we can integrate constraints by performing rejection sampling. Each inintial keyframe is sampled for collision free and constraint compliant points. After N number of such points are sampled, the distributions are refitted to this new sample set. This effectively shifts the keyframe distributions to become more representative of constraint-compliant collision-free configuration space.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 350px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/f21c0ce03b325ddd84f2b532eff69e05/b6a9b/Keyframing.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 100%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAIAAAAC64paAAAACXBIWXMAAAsSAAALEgHS3X78AAACCklEQVQ4y52Uy5aaQBCGef9nceEuixmTgw4qAkaEARRBUJTRqAhyh25TgJc5HMfMyb/obqr7q+pLFYSqqmEYnv9LBEVRruvCCGNcm0vTVNf1xWKxWq0sy4LB8qr5fA5GgqZpz/NqWOXI9/1uqUaj0W634TMpBetlSVYUhej3+xX8OXI1BjsEBMZxnOPxCEaEUDWbpLG5MJ/BgLEsCzF7vR5ssNVqvYJeXvfHnbp9n2nqMxjuAg4GA1QqLxVFEbRhHBim8QwOgoCiugzDCoLAcVyn07FKX0maa7Y/0/RnMDyhKApsqeFwCCtFUYSrNkxzLE01TavDuOjxzVEQpVmObrMII3i/LMswysFyh8+4aooO5t2jI01UeuqK+vbkuXnpop4ktcg5SsMokGWF50fWytY+Qnm2lCWJ53ljo/mp+zWMsb6fkOOftr2+ursc4c9h01F//TYH1cHusOteMsy27YHc1Q4KpMP9Fq5LvcThxgysuRkruNgM5BC8RxwnD2vgcv9B2KN7690Sn9EFPp1Oe8cbjfjKC8LoIV/ZzZ3enZM7/6OAmQFtrvfszDNs52Ft1RRnITeh7W2RLcSQY0eCwigbN8hK+BlZud5uttK7WMAkSbZJMs+y7/8DIG1YQT14AdFsNiH1iv3EcfovJUmK8mznnDrSYWJ5BCQ91BoUHfVtvb1RP17Iqar/BbaQX34WwDSJAAAAAElFTkSuQmCC&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;Keyframing&quot;
        title=&quot;Keyframing&quot;
        src=&quot;/static/f21c0ce03b325ddd84f2b532eff69e05/13ae7/Keyframing.png&quot;
        srcset=&quot;/static/f21c0ce03b325ddd84f2b532eff69e05/942f4/Keyframing.png 88w,
/static/f21c0ce03b325ddd84f2b532eff69e05/4edbd/Keyframing.png 175w,
/static/f21c0ce03b325ddd84f2b532eff69e05/13ae7/Keyframing.png 350w,
/static/f21c0ce03b325ddd84f2b532eff69e05/52211/Keyframing.png 525w,
/static/f21c0ce03b325ddd84f2b532eff69e05/b6a9b/Keyframing.png 530w&quot;
        sizes=&quot;(max-width: 350px) 100vw, 350px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;figcaption&gt;Clustering of points and keyframing&lt;/figcaption&gt;&lt;/figure&gt;
&lt;figure&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 350px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/a43d96f38e02d70fbe1e6ff685daae6f/b06ae/RejectionSampling.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 96.59090909090908%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAATCAIAAAAf7rriAAAACXBIWXMAAAsSAAALEgHS3X78AAACMUlEQVQ4y51TW3OaQBjl93c603/Ql751WmeaaWJskzAG5aIxRixeMFwkiBREBFSUi/QYpkYeGjM9Dzvf7n5nv7NnvyUEQYjjOPuL3ZuBZKJWq2232+y/QNA0fSC7rmsYxuwZtm0jNk0zj/PRcRzLsiYTw5gYCApkRVEqlUq5XCZJstVq9Xo9juMajQbDMEhjWRbrgtBbrgLHsyRJKpB1XR8MBuCMRiNVVSEE8XA4xDQPRFF8etKTNFklgaZpBTLm7XYbR/b7fRiJGMpzDg7FERg7nY4iK1mwGqtqgSzLcrPZhE4s1ut1eCnLEsvulUNzLh4JPM97v02cVSDDj+l06jwDDnmet3AXgDNzjrH30tyjQD5+bYy2YwkjfmbP3vRUB9o22m7CDXfP/mTPWY7xFh5y0l0axdFxIxXIotpTxlKWZrZtZbts8MhPTEXTxqDNHbfJ01fMue/7hxov5MBfXlbPbrnrxgMdbfcV2hQlMuw9TcdpqirqDV2p0tfL5bIge7PZIIqiCA9Ta92w7XqapFgJwxBPne+i4HRqrlfrwA+wEsfJS2XIwDZ+SBInSZS80s/zxQJGkMwP3I5AA05MnWZoSZFe4eQOpWlGXny9Ir+rmgpTiNZd667TYB+o9pBFkxzM+Bd8Z64p43Ad7mWjbcaqNnzsD0d9d+6e/NJIwO3mwNQkqtUqOulkwQPgpayLn758pG4viVKphNbFN/h1Ct1uFx+GoqjS+ef3H96dXXz7A8eeE0I/1PtTAAAAAElFTkSuQmCC&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;RejectionSampling&quot;
        title=&quot;RejectionSampling&quot;
        src=&quot;/static/a43d96f38e02d70fbe1e6ff685daae6f/13ae7/RejectionSampling.png&quot;
        srcset=&quot;/static/a43d96f38e02d70fbe1e6ff685daae6f/942f4/RejectionSampling.png 88w,
/static/a43d96f38e02d70fbe1e6ff685daae6f/4edbd/RejectionSampling.png 175w,
/static/a43d96f38e02d70fbe1e6ff685daae6f/13ae7/RejectionSampling.png 350w,
/static/a43d96f38e02d70fbe1e6ff685daae6f/52211/RejectionSampling.png 525w,
/static/a43d96f38e02d70fbe1e6ff685daae6f/b06ae/RejectionSampling.png 560w&quot;
        sizes=&quot;(max-width: 350px) 100vw, 350px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;figcaption&gt;Rejection Sampling to relearn constrained distributions.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;h4&gt;Culling&lt;/h4&gt;
&lt;p&gt;Once a dense sequence of keyframes is produce, the intermediate keyframe density is reduced to generate a more sparse model. It is important to note that keyframes representing when the set of applied constraints change are kept. &lt;/p&gt;
&lt;p&gt;&lt;figure&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 350px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/df0bc26b4b50a7998015134c3fac7e62/89a37/Culling.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 100%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAIAAAAC64paAAAACXBIWXMAAAsSAAALEgHS3X78AAAB3UlEQVQ4y52UbXeaMBTH+f5fZmcvdO2LbaUVBecYnuwEkQdJBDKfq05DEOiuxLN2LaU9uy8ChPzyvw+5UTzPO51OD/9liqqqnPPaf1mWUUrDF0YIgfkoipRut5um6TOsLEsYl8tlkiTyU1qe5zAKIYq8CIJA0TTtNXi73TqOo+u6YRimaQ4GA62y9XpN7j0/9JrgzWYDTs5ms/l8vlgs4CWOYxA8HI+i/E1o2ATvdjuEUKfTwRhLWYgW5ouiYJtsQmgTDO5BViBy0ARlkF2tVhJO1mkwaVTe7/eu6zHG4srki8w2S6Lx2KmHn+5TSZ3zDMcBxqIyeIDAc1iWRMr2TFsf+iGZ5q+con9gicHGh8MBnPyO3AHyp1GS8hRhZNkWKP9d9gIuSjZjdEpFKsqLzxdHIha1e+2hPZS718ATOvmofnB8R64oH+kzn7DYHmNIeD1smIaFLchGTd4qfQinfdf2qCdnHmGoJHRCQw9J3hr9uNY+8SO/KHN+hNrJ3irKorkNoWB0SqSeove62CXXnWGWiXe2cZad/CA4u313e3N18+2nGz28pfnkzO+/GvjXfKm0Wq3RaMyhUYTg6dsGedlt7z8bNnLC801yq6r9fh9uhd67DALVWldfEHb/AOTRY8jWFezjAAAAAElFTkSuQmCC&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;Culling&quot;
        title=&quot;Culling&quot;
        src=&quot;/static/df0bc26b4b50a7998015134c3fac7e62/13ae7/Culling.png&quot;
        srcset=&quot;/static/df0bc26b4b50a7998015134c3fac7e62/942f4/Culling.png 88w,
/static/df0bc26b4b50a7998015134c3fac7e62/4edbd/Culling.png 175w,
/static/df0bc26b4b50a7998015134c3fac7e62/13ae7/Culling.png 350w,
/static/df0bc26b4b50a7998015134c3fac7e62/52211/Culling.png 525w,
/static/df0bc26b4b50a7998015134c3fac7e62/89a37/Culling.png 532w&quot;
        sizes=&quot;(max-width: 350px) 100vw, 350px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;figcaption&gt;Sparsification step that maintains constraint boundary keyframes.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;h4&gt;Reconstruct Skill&lt;/h4&gt;
&lt;p&gt;Finally a skill is reconstructed by sampling in-sequence constraint-compliant waypoints from each keyframe distribution. Motion planning methods are then used to traverse from waypoint to waypoint.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 350px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/e46d2629a5f9b3ab75f1dc6f7aa4bde2/664c8/SkillExecution.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 100%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAIAAAAC64paAAAACXBIWXMAAAsSAAALEgHS3X78AAABb0lEQVQ4y+2TTW+CQBCG/e09NE2b/gxvHkxsIqSABAT6wccBI0sKrYkFa4EEpQHdBe3Y1dbaJhoPPXUOk83MPpl3ZmdrruuWZbk8ymqSJBFCjoRVVT0eVhTlH/6yxcaqqgL/J5XzPOc4zrIs0zQ1TZNlOU1TqmU/jDGGzev3++ARQrZtHwRvp9N00vPM8esLxgTa3gPTXBzHsHagFqpVy8U0m3blrm4YcRLt8N9gXM5uDEXsiFEUvWXZ02AwtO0HhPKiIKQUNNb10DZPYQwnSPPitdm7pwnQOZ/P4zAkGG90VTzPPw+DT34Fw5eEq6Io+r4PIfpDHcdhWZZhWUmWoQXac1HMGPFqkq2HV1MVdUZyTmJ875EWpHWglyRJgiAYhSEc6LZAfDQOOh1hLdswjDv9VrfuaM1qYzuDpUE6HcPUXc9Zwa1WC1T9fIbfF3a5+PDV6eUJclCtXq83Gg3Yp3a7zRxgcE3ghfOzi2az+Q7ThGxJOSKk8QAAAABJRU5ErkJggg==&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;SkillExecution&quot;
        title=&quot;SkillExecution&quot;
        src=&quot;/static/e46d2629a5f9b3ab75f1dc6f7aa4bde2/13ae7/SkillExecution.png&quot;
        srcset=&quot;/static/e46d2629a5f9b3ab75f1dc6f7aa4bde2/942f4/SkillExecution.png 88w,
/static/e46d2629a5f9b3ab75f1dc6f7aa4bde2/4edbd/SkillExecution.png 175w,
/static/e46d2629a5f9b3ab75f1dc6f7aa4bde2/13ae7/SkillExecution.png 350w,
/static/e46d2629a5f9b3ab75f1dc6f7aa4bde2/664c8/SkillExecution.png 524w&quot;
        sizes=&quot;(max-width: 350px) 100vw, 350px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;figcaption&gt;Exeuction of a constrained skill&lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;h2&gt;Limitations &amp;#x26; Future Work&lt;/h2&gt;
&lt;p&gt;A limitation of CC-LfD is it’s reliance on unconstrained motion planning. If an environment change introducing an intermediate keyframe occlusion, the motion planners cannot rely on the density of sequential keyframes to maintain constraint adherance. Our guess as to why constraint copmliance does occur in motion planning in the first place is because the density of keyframes effectively restricts the space of feasible motion plans to likely sit within the same space as constraint-compliant motion plans. This is also because the demonstration data to produce these keyframes generally show constraint-compliant paths. &lt;/p&gt;
&lt;p&gt;However, with occluded keyframes, the motion planning algorithms need to diverge away from the initial demonstration trajectory distributions in order to produce a collision-free motion plan. This divergence suddenly exposes the constaint naievete of unconstrained motion planners. Future work will look into integrating constrained motion planners into CC-LfD.&lt;/p&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;&lt;ol&gt;&lt;li&gt;&lt;b&gt;Robust Robot Learning from Demonstration and Skill Repair Using Conceptual Constraints&lt;/b&gt; &lt;br&gt;Mueller, C., Venicx, J. and Hayes, B., 2018. 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 6029--6036. &lt;/li&gt;&lt;li&gt;&lt;b&gt;Recent advances in robot learning from demonstration&lt;/b&gt; &lt;br&gt;Ravichandar, H., Polydoros, A.S., Chernova, S. and Billard, A., 2020. Annual Review of Control, Robotics, and Autonomous Systems, Vol 3. Annual Reviews.&lt;/li&gt;&lt;li&gt;&lt;b&gt;Keyframe-based learning from demonstration&lt;/b&gt; &lt;br&gt;Akgun, B., Cakmak, M., Jiang, K. and Thomaz, A.L., 2012. International Journal of Social Robotics, Vol 4(4), pp. 343--355. Springer.&lt;/li&gt;&lt;li&gt;&lt;b&gt;Robust Robot Learning from Demonstration and Skill Repair Using Conceptual Constraints&lt;/b&gt; &lt;br&gt;Mueller, C., Venicx, J. and Hayes, B., 2018. 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 6029--6036. &lt;/li&gt;&lt;/ol&gt;&lt;/bibliography&gt;&lt;/p&gt;
&lt;h5&gt;Footnotes&lt;/h5&gt;

      &lt;div class=&quot;footnotes&quot;&gt;
        &lt;hr/&gt;
        &lt;ol &gt;
    
    &lt;li class=&quot;footnote-list-item&quot; id=&quot;fn-1&quot; &gt;
          
        
      &lt;a href=&quot;#fnref-1&quot; class=&quot;footnote-backref&quot; style=&quot;display:inline;text-decoration: none;&quot;&gt;
        ↩
      &lt;/a&gt;
    &lt;p class=&quot;footnote-paragraph&quot; style=&quot;display:inline; margin-left: 5px;&quot;&gt;In kinesthetic demonstration, a human operator physically manipulates the robot to enable its sensors to capture a trajectory. In teleoperation, the operator utilizes a remote control device to manipulate the robot. In passive observation, the robot’s external sensors observe a human demonstrating the task. This last method introduces something called the &lt;strong&gt;correspondence problem&lt;/strong&gt;: how does one map observational demonstration data to robot control state data?&lt;/p&gt;
      &lt;/li&gt;
      
    

    &lt;li class=&quot;footnote-list-item&quot; id=&quot;fn-2&quot; &gt;
          
        
      &lt;a href=&quot;#fnref-2&quot; class=&quot;footnote-backref&quot; style=&quot;display:inline;text-decoration: none;&quot;&gt;
        ↩
      &lt;/a&gt;
    &lt;p class=&quot;footnote-paragraph&quot; style=&quot;display:inline; margin-left: 5px;&quot;&gt;CC-LfD falls under the policy learning category, where the constrained acyclic directed keyframe graph generates a rough motion plan for the robot to follow. The graph model represents a policy that dictates robot behavior.&lt;/p&gt;
      &lt;/li&gt;
      
    &lt;/ol&gt;&lt;/div&gt;</content:encoded></item><item><title><![CDATA[Developing the Natural Language Understanding Behind Shoppy Bot]]></title><link>https://www.carl-mueller.com/posts/developing-natural-language-understanding-for-shoppy-bot/</link><guid isPermaLink="false">https://www.carl-mueller.com/posts/developing-natural-language-understanding-for-shoppy-bot/</guid><pubDate>Thu, 26 Oct 2017 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;&lt;strong&gt;Lightning in a Bot’s first product, &lt;a href=&quot;https://shoppybot.com&quot;&gt;Shoppy Bot&lt;/a&gt;, is a natural language chatbot that Shopify store owners can use to query their business data.&lt;/strong&gt; The natural language interface is a novel approach to reporting and analytics in a backend retail setting, and one we hypothesis can provide faster and more efficient access to important data.&lt;/p&gt;
&lt;p&gt;The first version of Shoppy Bot used &lt;a href=&quot;https://wit.ai&quot;&gt;Wit.ai’s&lt;/a&gt; natural language API to convert user requests such as “how many customers have we had this week?” into structured data that we could convert into SQL queries and other various functions. Problems with these cloud services became apparent early on, therefore we began to explore what it would take to build our own Natural Language Understanding (NLU) service, one designed specifically for the task at hand.&lt;/p&gt;
&lt;p&gt;This post serves as an overview of the theory behind our approach and outlines specific implementation details in developing our own NLU system, now known internally as “Bolt”.&lt;/p&gt;
&lt;h2&gt;Theory&lt;/h2&gt;
&lt;h3&gt;Semantic Challenges&lt;/h3&gt;
&lt;p&gt;Semantics is a field of linguistics which concerns itself with the meaning of language, both written and spoken. Natural Language Understanding is a broad field that attempts to assign semantics to spoken or written language in order to derive understanding. Computer scientists, linguists, and machine learning engineers attempt to use this understanding as actionable information when developing applications. NLU presents unique challenges to engineers due to ambiguousness, variability in style and formation, context, prior knowledge dependency and the disparity between different languages themselves.&lt;/p&gt;
&lt;h3&gt;Intent-Slot Paradigm&lt;/h3&gt;
&lt;p&gt;One successful method that provides a good framework for generating actionable information from natural language is the &lt;a href=&quot;http://www.cs.toronto.edu/~aditya/publications/contextual.pdf&quot;&gt;intent-slot paradigm&lt;/a&gt;. The intent-slot paradigm is a method of Natural Language Understanding that combines a number of core Natural Language Processing and Information Extraction techniques in order to derive a semantic understanding of written language. In Shoppy Bot’s case, the derivation is a semantic understanding of the user’s question about their business data. There are three major types of information extracted under this NLU model: Intents, Slots, and Entities.&lt;/p&gt;
&lt;h4&gt;Intents&lt;/h4&gt;
&lt;p&gt;Intents are global properties of a text document (or user query) that map the document to an assigned goal or desire of the user. They can map to very broad concepts such as ‘weather’, ‘banking’, ‘location search’ or to specific concepts such as ‘get product info’, ‘schedule meeting, etc,. These intents are sometimes called user actions and it is the goal of Bolt to categorize queries into a set number of intents. The following query would be categorized to the given intent:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Query&lt;/strong&gt;: What is the shipping information for order #2314?&lt;br&gt;
&lt;strong&gt;Intent&lt;/strong&gt;: get-order-shipping-information  &lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;Slots&lt;/h4&gt;
&lt;p&gt;Slots are regions or spans within a text document, sometimes overlapping, that map to a specific type of information. They often constitute a semantically loaded region of the text.  An NLP system must correctly detect the right span/region that contains these semantic slots of information. The following date-time phrase provides an example of a slot:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Query&lt;/strong&gt;: How many returning customers did I have in [the first quarter of 2015]?&lt;br&gt;
&lt;strong&gt;Slot Type&lt;/strong&gt;: Date-time&lt;br&gt;
&lt;strong&gt;Extraction Information&lt;/strong&gt;: 1/1/2015 - 4/1/2015&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;Entities&lt;/h4&gt;
&lt;p&gt;Entities are also regions of text that contain semantic information but often boil down to a single word or compound. They can be thought of as a highly specific slot. Generally, entities encompass names, organizations, locations, and domain specific pieces of information. It is the goal of a named entity recognition system to identify these entities. For example:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Query&lt;/strong&gt;: Get me [Winona Snyder’s] most recent order.&lt;br&gt;
&lt;strong&gt;Entity&lt;/strong&gt;: Customer Name = Winona Snyder&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Implementation&lt;/h2&gt;
&lt;h4&gt;Model Generality vs Specificity&lt;/h4&gt;
&lt;p&gt;Often a goal of the design of an NLU system, especially one designed for a targeted domain, is to provide as much generality as possible. This often means that the system’s goal centers around supporting as many users and/or as many use cases as possible with a minimal amount of model generation and methods. The reason being is that the implementation of certain NLP features can be reused and shared. Share models are easier to maintain (no management of devoted models). Additionally, new training data generated by user behavior effectively crowd sourced. For example, in Bolt, the mechanism that determines a user’s intent is shared amongst all users. This way, new data (such as new variations of a similar query) can be incorporated into a newly trained model for which all users share the benefit.&lt;/p&gt;
&lt;p&gt;However, there are certain times when specificity takes precedence over generality. In our case, each one of our Shoppy Bot users has a store with a wide range of products, each with unique names. When parsing a query for slots and entities, an NLU system has to return information that is specific to that user. Avoiding returning information from one user to another is also a requirement as our customers expect their data and analytics to be protected. Bolt employs a fast dictionary/gazetteer algorithm that utilizes distinct data structures specific to each of our users.&lt;/p&gt;
&lt;p&gt;The following outlines three specific implementation details of the Bolt NLU system developed for Shoppy Bot in terms of the intent-slot NLU paradigm.&lt;/p&gt;
&lt;h4&gt;Intent Classification&lt;/h4&gt;
&lt;p&gt;Intent categorization is achieved most commonly through state-of-the-art text classifiers. Bolt uses a discriminative classifier known as a support vector machine. Briefly, support vector machines (SVMs) are classifiers that construct non-probabilistic models represented in a geometric manner. Data is mapped into a geometric space. The different categories or classes of data are separated by a hyperplane optimized for achieving the largest distance between the closest data points of two different categories. The technical details of support vector machines are beyond the scope of this post, but they are the soup du jour for text classifiers (especially for smaller datasets) and are fairly robust against outlier data.
Bolt uses the fantastic library Scikit-learn and its implementation of an SVM. Specifically we use the class &lt;a href=&quot;http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC&quot;&gt;LinearSVC&lt;/a&gt;. It supports multi-class classification and scales to larger datasets decently by employing &lt;a href=&quot;https://www.csie.ntu.edu.tw/~cjlin/liblinear/&quot;&gt;liblinear&lt;/a&gt; instead of &lt;a href=&quot;https://www.csie.ntu.edu.tw/~cjlin/libsvm/&quot;&gt;libsvm&lt;/a&gt;. One challenge is that many SVM’s underlying implementations, such as libsvm, are quadratic in their algorithmic complexity which can make datasets &gt;10000 samples highly inefficient. LinearSVC performs well as it does not use any kernel function to turn a dataset that is non-linear into a linearly interpretable dataset, thus dramatically enhancing the algorithmic efficiency.&lt;/p&gt;
&lt;p&gt;Any machine learning technique is powerless without data. Data must be curated in a way that best represents pertinent information about your problem. Bolt’s intent classification challenge is first and foremost a text classification problem. We take a user’s text query and classify that text into an intent category. Determining what type of data you give your machine learning algorithm is a process called feature generation. One of the most common ways to generate features (data tailored for use in a machine learning algorithm) from text is to employ a simple technique known as the ‘bag-of-words’ technique. The frequency (number of occurrences) of all the words for each text category are used as specific values for data points in the geometric space we generate. &lt;/p&gt;
&lt;p&gt;Similar words are used repeatedly for similar user intents. It plays to Bolt’s advantage that queries are often specific to the task at hand, which limits the amount of noise (overlap of categories) and number of outliers (single data points invading deep into the region of a different category). This means that the distinction between one category and another in our geometric space is sharp. I won’t go too deep into how SVM’s work, but I suggest Wikipedia as a good starting point.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 350px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/ee94b5d9c99d348b59a03549a5ce621b/0acb4/hyperplane.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 95.45454545454545%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAATCAYAAACQjC21AAAACXBIWXMAAAsTAAALEwEAmpwYAAAE3ElEQVQ4y2NgAIJJHT3iNY01wq2tzfxdbW18yLgTiDtAdHs7gt3WJtDd3i42Y+pUzmmTJjExQEFfdzcDQ9vsPu6G/hYZBhJAX1s708SeXrbVq1YyzZw2jWvW9Okc61evZgRLrlm4VNi3Jr7AJj8gP7oqXQQs1jOHtba5jrO3o5W9uW4qZ1XpLK7G2l6OzrZ2jo7Wbrb25nb+1sZ2gZamVpa25mY2oMt4JvX3C/f39gowzGzrFUgoyxB1Kgk7YlsSfNWpLKw5uT5P7M6szcwVtR3c//8zgL10/rIDS19vCfuyZSksHW2VfNOnp/BOm1LB2tHSwdne0swOMriro0OQoaG1nhekoXBKnZpDZfgJi+KAjxaF/vNg3ovy+qLq5/DY1cnimky4439mkFhK0GfdYNd7/uhB0dXZycXQ197Bk1aXzw4SCGpLtwUaet2yOOCFdVFwNUgs1PuBup/jo4wI9328wU53xYNdn5rH+d0PDXJ55uztcMMKpCY64DTYF0AX8jL0tLfz9HZ0MZc3VoJd6t2UlG5XGfYQaOhDu5KwAJCYj/NxXQvzS172gf+FswLuc61ZUc6ZEvzROdzjSaqn3QUOkJrmhkksYBd2AQ1cXl7K2ldfyw1zuldjfI9tRegzy+LAy65VwRq2uv+Vtli5SGXZHQy1t3lgFupxwc3Z8rgssne7uzrZujs7ORmmNNRzM/z/D3byDwcLu/cmBr4gtkd97Hqb8pAXNiUBOw18Llne07U3nGLdEmjg8lUtyOWQu7f9PS0fhwduCUHvwC7saOvlBLqQjQFoGPMTbzfLC872Sj8drHQvOFpKgBRkdhWoOtfFHrcuC35pU+41CSRmYvdcNdT9mkFs8BlpEN/X8YqGj/0tc4iXJ/L1dHczMqzs7+N56WKXeszHA+yF9/HhnG9c7ML/m+l7TstwLnCsibplUR7w2D6jsLuW4T9XQtp8EX+Xi1Go8fsf6OUOno72dkaGiQ0NPGrXHzK9crH3uxrsr34/wNfivbNpyA7vbK9c/3+WXi0xOQ4lvi8sciPeu6SVZIK0JyetlAMZUla4jBESfm0s3Z1dXGCzqyZO4jyXGMv71Mbc95eLrdJ7S+PeD3b6Oo8cA1232FcEgdT4V0dNt6kKempR6nvaujBMBSRW2lfIgoiQLpbOjg6IgdOqKrlA4XjPw9n4i5GO8Etbc9Uzmto8YI84ajp9MjdKWNRYLOjSHL/euiLkDTDRb06uL+VbP38ZBzxBd3RwAmOYFcxZVF/Hkrp4Kff1AG+Fd8a6YiCxXx4OUl9sLVyeWZhZ1+WEs4HE4iYXqzo3xpyyLAt6Y1MS7AZ2ZWcNOEMADeMBuhJS6swrLmSumDVb6IuDle0tQ11wWnxnZ2H40txQYZehLtiw8NIQsPeCerNcHOoiX9tWhtaBXTapmw9qIDfQy5DSZnp5OXPDlKkib61N1a6YGXGh58+DPp6MU4ElhHGxLzgfe7UnVTrWRx0AO2bGDI7JEydwtre1gfUBaQaG3v5+5t6WFnHURADBuIBne9Jkl5Y4IxB74uR+YaDrwD7r6epiYCgvLASVaRLrGupZO2trWbp7elgmTZzI1lBfz9XW2soDLKF5gDaD6caOZr4zFStZ0/vKVKN78u1vN2xjaW5tFunu6AAHCTAbMwAAMnXbuLftc78AAAAASUVORK5CYII=&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;hyperplane&quot;
        title=&quot;hyperplane&quot;
        src=&quot;/static/ee94b5d9c99d348b59a03549a5ce621b/13ae7/hyperplane.png&quot;
        srcset=&quot;/static/ee94b5d9c99d348b59a03549a5ce621b/942f4/hyperplane.png 88w,
/static/ee94b5d9c99d348b59a03549a5ce621b/4edbd/hyperplane.png 175w,
/static/ee94b5d9c99d348b59a03549a5ce621b/13ae7/hyperplane.png 350w,
/static/ee94b5d9c99d348b59a03549a5ce621b/0acb4/hyperplane.png 392w&quot;
        sizes=&quot;(max-width: 350px) 100vw, 350px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;figcaption&gt;Hyperplane separating two categories&lt;/figcaption&gt;&lt;/figure&gt;
&lt;em&gt;A hyperplane (which is a standard flat plane in three dimensions) separating two categories distinguished by red and blue spheres. &lt;a href=&quot;https://i.stack.imgur.com/zeRTm.png&quot;&gt;Source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;Slot Detection&lt;/h4&gt;
&lt;p&gt;Slot detection sometimes blurs the line between entity parsing and recognition. It can help to think of named entity recognition (NER), where one parses a product name, city and state etc., as a subset of slot detection. Slots can be a feature of the entire piece of text being analyzed or a localized span of text. One example of a slot that spans the entirety of a user query would be plurality. For example, if the user asks, “Who is my top customer?”, Bolt is able to recognize that the person is asking for only one customer, without needing any explicit number to exist in the query. These results are passed back to Shoppy Bot where a response is sent to the user with the results for one customer. &lt;/p&gt;
&lt;p&gt;Bolt actually uses a binary classifier to determine the query’s plurality. We use a similar ‘bag of words’ technique as the intent classifier but also employ a technique that emphasizes particular features. The emphasized features are words associated with plurality. For example, verb conjugation plays an obvious role in identifying whether or not the query is asking for plural results. The verbs ‘is/are’, ‘was/were’ greatly increase the accuracy of the classification model without the need for large amounts of data. It is a case where smart feature engineering helped defeat a lack of data on our end. As the slot detection of plurality is independent of any specific user, it is a generalizable model.&lt;/p&gt;
&lt;h4&gt;Named Entity Recognition&lt;/h4&gt;
&lt;p&gt;Named entity recognition is the probably the most well-known aspect of information retrieval and slot detection in terms of NLP. Shoppy Bot users each have a large set of product names associated with their Shopify store. These “named entities” must be identified and parsed specific to the user. Bolt employs a set of data structures and a specialized string comparison algorithm to successfully parse product names from a user’s query.&lt;/p&gt;
&lt;p&gt;The data structure that Bolt’s gazetteer uses is called a trie. In this scenario, a trie is an ordered tree structure that stores strings compactly, where each node of the tree is an object or structure that stores the current letter, potentially a word, and references or pointers to the next node. The idea is that in traversing a branch of the trie, you slowly build up a word letter by letter. Words that share the same prefix will start along the same branch.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;&lt;span
      class=&quot;gatsby-resp-image-wrapper&quot;
      style=&quot;position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 350px; &quot;
    &gt;
      &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/037763317989df7427c7fac152dedcb9/6a068/search_trie.jpg&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
    &lt;span
    class=&quot;gatsby-resp-image-background-image&quot;
    style=&quot;padding-bottom: 75%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAPABQDASIAAhEBAxEB/8QAFwAAAwEAAAAAAAAAAAAAAAAAAAECBf/EABQBAQAAAAAAAAAAAAAAAAAAAAD/2gAMAwEAAhADEAAAAd1UigD/xAAWEAEBAQAAAAAAAAAAAAAAAAAQEQD/2gAIAQEAAQUCZof/xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/AT//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/AT//xAAUEAEAAAAAAAAAAAAAAAAAAAAg/9oACAEBAAY/Al//xAAZEAEBAQADAAAAAAAAAAAAAAABABExQXH/2gAIAQEAAT8hXSOe4dkXqy//2gAMAwEAAgADAAAAEDQP/8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPxA//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPxA//8QAGxABAQADAQEBAAAAAAAAAAAAAREAITFBgZH/2gAIAQEAAT8Q5jC9qY4BW+C9/cCp4xxBojEuW04gc3rAH3bn/9k=&apos;); background-size: cover; display: block;&quot;
  &gt;&lt;/span&gt;
  &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        alt=&quot;search trie&quot;
        title=&quot;search trie&quot;
        src=&quot;/static/037763317989df7427c7fac152dedcb9/70ebb/search_trie.jpg&quot;
        srcset=&quot;/static/037763317989df7427c7fac152dedcb9/f71f9/search_trie.jpg 88w,
/static/037763317989df7427c7fac152dedcb9/e52aa/search_trie.jpg 175w,
/static/037763317989df7427c7fac152dedcb9/70ebb/search_trie.jpg 350w,
/static/037763317989df7427c7fac152dedcb9/7349d/search_trie.jpg 525w,
/static/037763317989df7427c7fac152dedcb9/29d31/search_trie.jpg 700w,
/static/037763317989df7427c7fac152dedcb9/6a068/search_trie.jpg 960w&quot;
        sizes=&quot;(max-width: 350px) 100vw, 350px&quot;
        style=&quot;width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;&quot;
        loading=&quot;lazy&quot;
      /&gt;
  &lt;/a&gt;
    &lt;/span&gt;&lt;figcaption&gt;String search trie&lt;/figcaption&gt;&lt;/figure&gt;
&lt;em&gt;&lt;a href=&quot;http://images.slideplayer.com/32/9814580/slides/slide_5.jpg&quot;&gt;Source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;This data structure allows one to check if a word is in a dictionary extremely quickly but also allows for additional algorithms to function during the search process. Hash lookups for words are generally the fastest mechanism for dictionary lookups. However, if you want to perform string similarity comparisons while searching for a word in a dictionary at the same time, the search trie is your best bet. This allows for robust dictionary lookup of words pulled from the query to see if they match (or are close to matching) the words stored in the trie. Ultimately this is a very robust way to perform fuzzy dictionary lookups. &lt;/p&gt;
&lt;p&gt;The string similarity mechanism Bolt’s gazetteer uses is the &lt;a href=&quot;https://people.cs.pitt.edu/~kirk/cs1501/Pruhs/Spring2006/assignments/editdistance/Levenshtein%20Distance.htm&quot;&gt;Levenshtein&lt;/a&gt; string similarity algorithm. Briefly, it is an algorithm that outputs an integer value called the string distance representing the number of insertions, switches, and deletions it takes to convert one word into another. After removing common words and certain specialty words from the query, the remaining words are grouped in a number of combinations. These combinations of words are each run through the gazetteer’s search algorithm to find matches within a certain Levenshtein string distance. The algorithm traverses down the trie building test words from the letters along the nodes in the path. The algorithm can dynamically append or remove new letters from the current current trie word so that the Levenshtein algorithm only has to readjust for the new letter in its comparison with the test word from the query. Thus we are not constantly performing and reperforming the entire Levenshtein algorithm for every word in the trie.&lt;/p&gt;
&lt;p&gt;One additional step Bolt takes to ensure robustness is to add n-grams and skip-grams of the product names into the trie. For example, ‘sally sells sea shells’ as n-grams of length 3 (tri-grams) would be split into ‘sally sells sea’ and ‘sells sea shells’. Skip-grams of length skip distance 1 would result in ‘sally sea shells’ and ‘sally sells shells’. The purpose of these n-grams and skip grams is to account for the different ways a user might express a product name. Perhaps they don’t type every single word if it is a long product name.&lt;/p&gt;
&lt;h4&gt;The Big Picture&lt;/h4&gt;
&lt;p&gt;Bolt first classifies the user’s query to an intent. Each intent is associated with a set of possible slots/entities that can be parsed for that intent. Those slots/entities are then parsed. In summary, Bolt generally operates with the following procedure:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Classify User Query to Intent:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Query&lt;/strong&gt;: What is the shipping information for order #2314?&lt;br&gt;
&lt;strong&gt;Intent&lt;/strong&gt;: get-order-shipping-information&lt;br&gt;
&lt;strong&gt;Associated Slots/Entities&lt;/strong&gt;: Order Number  &lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Perform slot/entity detection&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Query&lt;/strong&gt;: What is the shipping information for order #2314?&lt;br&gt;
&lt;strong&gt;Entities&lt;/strong&gt;: Order Number = #2314&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;Return information to Shoppy Bot server&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;This post provided a fairly indepth overview of some of the theory and implementation details that go into making the Bolt NLU system function. Natural language understanding is exploding and becoming more advanced day by day. The intent-slot paradigm for NLU is a robust and relatively simplistic approach to NLU that can be achieved fairly easily with some simple and clever algorithms and engineering. With this approach, we’re able to provide an NLU system that outperforms other cloud-based solutions on the market, and offer a more sophisticated chatbot experience.&lt;/p&gt;</content:encoded></item></channel></rss>