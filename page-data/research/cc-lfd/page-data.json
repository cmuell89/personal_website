{"componentChunkName":"component---src-templates-blog-post-js","path":"/research/cc-lfd/","result":{"data":{"markdownRemark":{"id":"fdc4f90d-32ea-59c4-9749-1e36e6f5d2fa","html":"<h2>Why is this important?</h2>\n<p>My PhD research’s seminal work is called <strong>Concept Constrained Learning from Demonstration</strong> <span id=\"citation-0\" data-hover=\"\"><span class=\"citation-number\">[1]</span></span>. It is this work that motivates my current research into constrained motion planning and human-robot interfaces, specifically augmented reality interfaces. Ultimately, I want to figure out how to make robots easily trainable and usable by non-roboticists. Physical automation is a wide-open frontier in the world of information technology. However, the introduction of collaborative robots into human environments presents a number of challenges often not required of large-scale industrial robots: safety in shared workspaces, rapidly changing task requirements, decision-making, and, perhaps most challenging, adhering to human expectations of behavior. As such, the foundational motivation behind this work is to provide human users the means to easily train collaborative robots to execute dynamic skills while adhering to important behavioral restrictions.</p>\n<h2>Background</h2>\n<h3>Learning from Demonstration</h3>\n<p>Robot Learning from Demonstration (LfD) consists of methods that attempt to learn successful robot behavior models from human input. A human operator interacts with a robotic system through some mode of demonstration, usually through kinesthetic demonstration, teleoperation, or passive observation <sup id=\"fnref-1\"><a href=\"#fn-1\" class=\"footnote-ref\">1</a></sup>. Demonstration ideally communicates information about the nature of the skill that the robotic learning system uses to build a learned model that resembles some latent (i.e. hidden) ground truth model. The methods by which robotic systems learn such models spans across the spectrum of machine learning. However there are three broad categorizations for robot LfD systems: 1) plan learning, 2) functional optimization, and 3) policy learning  <span id=\"citation-0\" data-hover=\"\"><span class=\"citation-number\">[2]</span></span>.</p>\n<p><figure><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 350px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/4d3b3c608170205bce549ef23656749a/3acf0/ActionShot.jpg\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 55.68181818181818%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAALABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAAAAMCBP/EABYBAQEBAAAAAAAAAAAAAAAAAAABAv/aAAwDAQACEAMQAAABtKW805h//8QAHBAAAgEFAQAAAAAAAAAAAAAAAgMTAAEREiIy/9oACAEBAAEFAm9U8hiWBajfLHAMI+f/xAAWEQEBAQAAAAAAAAAAAAAAAAAAARH/2gAIAQMBAT8Bka//xAAWEQADAAAAAAAAAAAAAAAAAAAAARH/2gAIAQIBAT8BbhD/xAAaEAACAwEBAAAAAAAAAAAAAAAAAQIRIRKh/9oACAEBAAY/AuBQXpsRpl1tCP/EABwQAQACAgMBAAAAAAAAAAAAAAEAESExQVGBsf/aAAgBAQABPyF0e0cvqvEZZ8uKOIzywv5GW6GY3cmJ/9oADAMBAAIAAwAAABBrH//EABcRAAMBAAAAAAAAAAAAAAAAAAABEVH/2gAIAQMBAT8QoqRh/8QAFxEAAwEAAAAAAAAAAAAAAAAAAAERQf/aAAgBAgEBPxC6Dd6f/8QAGxABAAMBAAMAAAAAAAAAAAAAAQARIVExQXH/2gAIAQEAAT8QoIrI6/dm3J2eB18seg2Oo4qsj4hVOagPkRT1DGABg1P/2Q=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"ActionShot\"\n        title=\"ActionShot\"\n        src=\"/static/4d3b3c608170205bce549ef23656749a/70ebb/ActionShot.jpg\"\n        srcset=\"/static/4d3b3c608170205bce549ef23656749a/f71f9/ActionShot.jpg 88w,\n/static/4d3b3c608170205bce549ef23656749a/e52aa/ActionShot.jpg 175w,\n/static/4d3b3c608170205bce549ef23656749a/70ebb/ActionShot.jpg 350w,\n/static/4d3b3c608170205bce549ef23656749a/7349d/ActionShot.jpg 525w,\n/static/4d3b3c608170205bce549ef23656749a/29d31/ActionShot.jpg 700w,\n/static/4d3b3c608170205bce549ef23656749a/3acf0/ActionShot.jpg 2000w\"\n        sizes=\"(max-width: 350px) 100vw, 350px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span><figcaption>A human user (me) kinestheticaly demonstrating a skill.</figcaption></figure></p>\n<p>The ultimate goal of these learning methods is to facilitate the transfer of information from a non-roboticist, with some expert intuition about the skill, to the robotic learning system. This information is then used by robot skill learning methods to produce successful learned models of the task. Plan learning methods attempt to learn models that operate at high levels of task abstraction, either learning a primitive sequence or hierarchy. Functional optimization methods either directly optimize a candidate trajectory (potentially one derived from demonstration) using a known objective function, or they attempt to learn an objective from demonstration. These approaches often emulate or directly draw from Reinforcement Learning and Inverse Reinforcement Learning techniques. Lastly, policy learning methods produce models that output either trajectories or low-level actions directly <sup id=\"fnref-2\"><a href=\"#fn-2\" class=\"footnote-ref\">2</a></sup>.</p>\n<h3>Keyframe Learning from Demonstration</h3>\n<p>CC-LfD is an augmentation of a learning method called Keyframe LfD (KLfD) <span id=\"citation-0\" data-hover=\"\"><span class=\"citation-number\">[3]</span></span>. In traditional KLfD, human operators teach a skill by providing distinct waypoints of robot state data. This represents a coarse trajectory for the robot to follow. This approach is powerful because it very easily allows users to specify robot motion, but it is somewhat brittle as the learned skill is really a concrete instantiation of one robot trajectory. Any variation to the environment or to changes in user expectations cannot be accommodated.</p>\n<p><figure><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 350px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/c0c8ce24e8bfeb84899aa7e53e331d58/064b2/Akgun_Keyframe_LfD.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 32.95454545454546%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAHCAIAAACHqfpvAAAACXBIWXMAAAsSAAALEgHS3X78AAAA8ElEQVQY0zWR2W6FMAxE8///WGhBBSEC2cnCdk8T1Q/WeLyM4whTTSnlnDvPM4RgrfXeA85SUorTKLdVLUbHnK/r2vd9miYpJWWC4L7vnDNxSolBVMDQH2P03mkd3vf9ldL4UHKmnuLjOOZ5FsuyaK2hkIIFM6LhUsq+k7LOR6ksYetBc9s2pot1XWGRep4HKRJ3NdTg2f6I6apZNoJhNaa3UCAFxWvxqNEMuP5Dxq/Wk2WPlP+M66DByziHaEsabdqercE6Bw/GU4oAvhUA6LfW0SW+q43j+FOt73v8MIxd1/XVSFU/ALrui1O3v+B3Pj2/jm6z6+44AAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Akgun Keyframe LfD\"\n        title=\"Akgun Keyframe LfD\"\n        src=\"/static/c0c8ce24e8bfeb84899aa7e53e331d58/13ae7/Akgun_Keyframe_LfD.png\"\n        srcset=\"/static/c0c8ce24e8bfeb84899aa7e53e331d58/942f4/Akgun_Keyframe_LfD.png 88w,\n/static/c0c8ce24e8bfeb84899aa7e53e331d58/4edbd/Akgun_Keyframe_LfD.png 175w,\n/static/c0c8ce24e8bfeb84899aa7e53e331d58/13ae7/Akgun_Keyframe_LfD.png 350w,\n/static/c0c8ce24e8bfeb84899aa7e53e331d58/52211/Akgun_Keyframe_LfD.png 525w,\n/static/c0c8ce24e8bfeb84899aa7e53e331d58/8c557/Akgun_Keyframe_LfD.png 700w,\n/static/c0c8ce24e8bfeb84899aa7e53e331d58/064b2/Akgun_Keyframe_LfD.png 1874w\"\n        sizes=\"(max-width: 350px) 100vw, 350px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span><figcaption>A visual representation of keyframe learning from demonstration (Akgun, 2012) where trajectories that outline a ‘P’ are clustering into keyframes.</figcaption></figure></p>\n<p>Keyframe LfD can be made more robust through automating keyframe generation and through statistical learning. To automate this approach, users first provide high-rate-of-sampling demonstration trajectories of the skill, ideally expressing subtle variation. Demonstration trajectories are aligned using a technique called Dynamic Time Warping <span id=\"citation-0\" data-hover=\"\"><span class=\"citation-number\">[?]</span></span>, which is an algorithmic method to align similar regions in sequential data from one sequence to another. The data points of these temporally aligned demonstration trajectories are clustered into sequential groups across demonstrations. These clusters of robot state data are fitted to learned <em>keyframe distributions</em>, which are used to generate waypoints that the robot follows sequentially to perform a skill. Forming statistical distributions to represent keyframes, as opposed to single points, enables the LfD algorithm to adapt to behavioral restrictions the human operator might decide to place on the robot.</p>\n<h2>Concept Constrained Learning from Demonstration</h2>\n<p>To this end, an algorithm called Concept Constrained Learning from Demonstration integrates behavioral restrictions, communicated by the user, into the keyframe LfD model. These restrictions are dubbed ‘concept constraints’ which represent prohibitions on the behavior of the robot system. This is accomplished by encoding concept constraints as Boolean planning predicates which when combined into logical forumlae and represent a multitude of concurrent constraints on the learning of keyframes. These planning predicates serve as rejection samplers during the distribution learning phase as well as the skill reconstruction phase. </p>\n<h3>Algorithm Overview</h3>\n<p>The algorithm follows these broad steps (right to left in the image cluster): </p>\n<ol>\n<li>Recording of Demonstration Trajectories and Constraints</li>\n<li>Alignment of Trajectories</li>\n<li>Clustering and Rejection Sampling Models</li>\n<li>Culling of Keyframes</li>\n<li>Remodeling</li>\n<li>Skill Execution</li>\n</ol>\n<h4>Recording of Demonstration Trajectories</h4>\n<p>The first step in CC-LfD, as with most LfD algorithms, is to capture demonstration trajectories provided by human operators. In CC-LfD, robot demonstration trajectories are captured via kinesthetic demonstration. What is unique to the CC-LfD demonstration process is that human teachers can provide constraint annotations. An annotation indicates the spatio-temporal region of a trajectory where a given constraint must hold true. In the published version of this work <span id=\"citation-0\" data-hover=\"\"><span class=\"citation-number\">[1]</span></span>, this constraint annotation was done manually through programmed arm cuff buttons on the robot during experimentation. However, future work plans to use an augmented reality interface or to utilize a natural language interface. </p>\n<p><figure><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 350px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/0044ff6f28d0645e64f8278a05838d70/a58fe/Demonstration.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 97.72727272727272%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAIAAAAC64paAAAACXBIWXMAAAsSAAALEgHS3X78AAACfElEQVQ4y31T2W7aQBT1l/YD+gH9AB5oK+UZ8UCR4KVSGgqUlpQS1rDv+1ow2CxR2AwUQ8A2PZ5JDKlQj2Rr5s7dzpk7zHA43G63x+Nxs9lUq9VerzcajYYEWPT7/R4By/YGPDd5GFILy7LNZpOJRqOz2QzBq9UK1k6nU6vVcrlcsVjMZrOTyQRHiqLgPxPWwRInSdKRAFmYZDK5WCywWS6XtBpL8JtAEAQcSbKseo8eP7lzori9EIzKHMehICyJRMLn8wWDQWTBkUyC1fqKTHGh8ng81pr8D6jDq+D1eg3OqVQqHo+DbYIgQtBut+EAFuFw2Gq1Qips4awGz+dzWnkwGIDkdDqFTo8EDwTIC53K5TJqmkwmj8cD/263e6r89PSE/YCAKkeBLbSgV8jzPFIjHbSA/RSsENDF/gXaxVDZcAQLFQx4FUybAVVwuycAYdx5qVSCllpqTbZTMJBOpxFJ75bicDigSfQMCf1+P+icx58EazQaiLx4VdQiimIgEKhUKpqFoaXAxOv1guR5DOH4PBZaxlAoRCdHbRu3ipSYZ7wKmKYEuBtxuz0vqwmGAhg+0HluG96xaGS1XNKRyGdS198DHz7Hspn0QhA0bbVE9XodEqrBELNaKf8KRt13IZ7rU6c/4q7VH0MnKvhutztvAU8Yzatt5/MF788fV9bbbzEMnSKdMaR/juPvQwFF2r/UVo14yKDGILFOp2u1WurTk2isLBFgsSfcvtxlnJHaUTlIMj51bAqFApRmjEajwWDQ3t0/kEmddJV98/bdTtzQAvg7HA69Xs+43W6z2exyuWw229dLsNsddtv11cf3NzeqA9ycTqfFYgHtv1SdNn/PFKnjAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Demonstration\"\n        title=\"Demonstration\"\n        src=\"/static/0044ff6f28d0645e64f8278a05838d70/13ae7/Demonstration.png\"\n        srcset=\"/static/0044ff6f28d0645e64f8278a05838d70/942f4/Demonstration.png 88w,\n/static/0044ff6f28d0645e64f8278a05838d70/4edbd/Demonstration.png 175w,\n/static/0044ff6f28d0645e64f8278a05838d70/13ae7/Demonstration.png 350w,\n/static/0044ff6f28d0645e64f8278a05838d70/52211/Demonstration.png 525w,\n/static/0044ff6f28d0645e64f8278a05838d70/a58fe/Demonstration.png 548w\"\n        sizes=\"(max-width: 350px) 100vw, 350px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span><figcaption>Recording and constraint annotation</figcaption></figure></p>\n<h4>Alignment</h4>\n<p>After capturing demonstration trajectories, an important step is to generate a mapping between the points of one trajectory with points in the other. Under the assumption that a human operator demonstrates repeatedly the same skill, drawing from a consistent skill distribution, it is important to be able to know which regions of one trajectory correspond with regions in another.</p>\n<p><figure><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 350px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/230a2bc27b98ebc2de4172a8486c38c4/432e7/Alignment.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 96.59090909090908%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAATCAIAAAAf7rriAAAACXBIWXMAAAsSAAALEgHS3X78AAACgUlEQVQ4y31UyXLaQBDlr+M/4ANcRYWqXInPPsVVgFjMbmF2UxSrJBMWIdAGGmkkyBtNTGTHSR9GrZ5+09OvnxRbr9eEkEvEFovFfD7f7XaqqmqatlqtZFlWFOWVmfKqKMvlcjKZHI/HWLVa3e/3wJzPZw5uNBq1Wq3ZbIqimMvler0egkHguy4lLnWIFwQXnHg4HGJPT094RMGdTqfVao1Go263WyqV4KAI4gfLMWxysIjl+NvtVtf1GOp8AD8/P1cqFcTzoWUymUKhcN3lzudgrOgZG6gGLvzQooz8DwxDI+g5nU7X6/Vo3Dk5J/tom1bgB+DSMIxPwGAItGIbg0D905tZpmUapqEbxCGSLDHCrmCOh2FI4F8PDddbhYbxsMc6XNZrDA9NvavMizuO8zM05BHiXP5t7ypDEjkhhwhGhXXQ75fKVVBdLpen06lHvXDgwbXMH7Bt2w8/HpbKK3xKqeuyVFW34WALBwlZAReOssPAXGE4XpFliAj+ZrOh1DOPkAS7NnID6pu6KQgCmr/iGRgHg5h0Jo3QKRQTV7umnzxK2atDDrs9j2OEyOf4WL1Rty1bFJuqup1Np8V8oVyptNvt1UaFGJEEkexV7Qxx+z64HI/HEMJvMMQITLPVromdcqnkeaxDltFs5/LFXre726hIRRwj5Dd6fHzE2Nm18RmAz/v7+/7gJUoG96WFlM1kQYFlWbwy2IbyZrMZA6NyIpHAYZxkLmb65iEIhvBtoaam7TEX4roQwGAwYOC7u7t4PP6h5tVQB+vX5LdiRYR3JNQLLnN5dXPzBd3FUqkU7gy5DYfD0V+GoCRJt7e3qdR3/EyGw5fJeAwJJZNJnPsLjzsPnBQ0NIYAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Alignment\"\n        title=\"Alignment\"\n        src=\"/static/230a2bc27b98ebc2de4172a8486c38c4/13ae7/Alignment.png\"\n        srcset=\"/static/230a2bc27b98ebc2de4172a8486c38c4/942f4/Alignment.png 88w,\n/static/230a2bc27b98ebc2de4172a8486c38c4/4edbd/Alignment.png 175w,\n/static/230a2bc27b98ebc2de4172a8486c38c4/13ae7/Alignment.png 350w,\n/static/230a2bc27b98ebc2de4172a8486c38c4/52211/Alignment.png 525w,\n/static/230a2bc27b98ebc2de4172a8486c38c4/432e7/Alignment.png 570w\"\n        sizes=\"(max-width: 350px) 100vw, 350px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span><figcaption>Trajectory alignment using DTW.</figcaption></figure></p>\n<p>One important caveat in this alignment process is that the algorithm takes special note of constraint annotation boundaries, regions in spatio-temporal regions in a trajectory where the set of applied constraints changes.</p>\n<p><figure><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 350px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/1ff9e36847d86ddf6447e406e0194c18/e40ed/DTW.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 50%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAYAAAC0VX7mAAAACXBIWXMAAAsSAAALEgHS3X78AAABg0lEQVQoz61SS07DMBDN0bgEa9ZsS4OQEFwBFpCABIg7INpu2HEDJFAlAkGgNknT1k7iz3jshEk/YgEskBjpad7z2JbHb7yqqg6stQkiPhKGP4HqQ7vOa76uLfFkLb4ZYwae1jpo/hL17yXnXOQppY5XWtR1DZSBzixyC2cM6KIE1AA4ycFWFUhJQrUAgCVXFrBx1j60L7xu/inoQROPcbYhs2y/HCcdlmYdNk66PHrxRZruiDjenUevfvUc7ako8mWS7Ehap/1LpGm31SqOfTUa7ZEfWx4vik1TFNdmOj0xnJ+aaR5iPgnLLA/K949zOx6dyfjtHObz0DAWIOcBzmYBFkVg8jwE4pBlATB2oaU89OgHrv6tZefS9g+PFkII0UgJNaHRGsycgUMEGpOFSY4MA2NBA4IgM6QyoCgrEkimNLgyBbQ+ay80dE9j3cr/5ov/Iax1scc536a274WQN4yXfSHVQArZI93XWg1oWHvURZ/G6xunoem1exDNLem7SojLT6CB7nloDitRAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"DTW\"\n        title=\"DTW\"\n        src=\"/static/1ff9e36847d86ddf6447e406e0194c18/13ae7/DTW.png\"\n        srcset=\"/static/1ff9e36847d86ddf6447e406e0194c18/942f4/DTW.png 88w,\n/static/1ff9e36847d86ddf6447e406e0194c18/4edbd/DTW.png 175w,\n/static/1ff9e36847d86ddf6447e406e0194c18/13ae7/DTW.png 350w,\n/static/1ff9e36847d86ddf6447e406e0194c18/52211/DTW.png 525w,\n/static/1ff9e36847d86ddf6447e406e0194c18/8c557/DTW.png 700w,\n/static/1ff9e36847d86ddf6447e406e0194c18/e40ed/DTW.png 1378w\"\n        sizes=\"(max-width: 350px) 100vw, 350px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span><figcaption>An example of an alignment of two sequences using Dynamic Time Warping.</figcaption></figure></p>\n<h4>Clustering and Rejection Sampling Modeling</h4>\n<p>The bulk of the algorithm’s novelty lies in the way it facilities the learning of keyframes. Once an alignment is completed, sequential clusters of aligned points serve as the basis for keyframe models. Each cluster of points are fitted with a statistical distribution (in this case Kernel Density Estimation). By modeling waypoints as distributions, rather than single points, we can integrate constraints by performing rejection sampling. Each inintial keyframe is sampled for collision free and constraint compliant points. After N number of such points are sampled, the distributions are refitted to this new sample set. This effectively shifts the keyframe distributions to become more representative of constraint-compliant collision-free configuration space.</p>\n<p><figure><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 350px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/f21c0ce03b325ddd84f2b532eff69e05/b6a9b/Keyframing.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 100%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAIAAAAC64paAAAACXBIWXMAAAsSAAALEgHS3X78AAACCklEQVQ4y52Uy5aaQBCGef9nceEuixmTgw4qAkaEARRBUJTRqAhyh25TgJc5HMfMyb/obqr7q+pLFYSqqmEYnv9LBEVRruvCCGNcm0vTVNf1xWKxWq0sy4LB8qr5fA5GgqZpz/NqWOXI9/1uqUaj0W634TMpBetlSVYUhej3+xX8OXI1BjsEBMZxnOPxCEaEUDWbpLG5MJ/BgLEsCzF7vR5ssNVqvYJeXvfHnbp9n2nqMxjuAg4GA1QqLxVFEbRhHBim8QwOgoCiugzDCoLAcVyn07FKX0maa7Y/0/RnMDyhKApsqeFwCCtFUYSrNkxzLE01TavDuOjxzVEQpVmObrMII3i/LMswysFyh8+4aooO5t2jI01UeuqK+vbkuXnpop4ktcg5SsMokGWF50fWytY+Qnm2lCWJ53ljo/mp+zWMsb6fkOOftr2+ursc4c9h01F//TYH1cHusOteMsy27YHc1Q4KpMP9Fq5LvcThxgysuRkruNgM5BC8RxwnD2vgcv9B2KN7690Sn9EFPp1Oe8cbjfjKC8LoIV/ZzZ3enZM7/6OAmQFtrvfszDNs52Ft1RRnITeh7W2RLcSQY0eCwigbN8hK+BlZud5uttK7WMAkSbZJMs+y7/8DIG1YQT14AdFsNiH1iv3EcfovJUmK8mznnDrSYWJ5BCQ91BoUHfVtvb1RP17Iqar/BbaQX34WwDSJAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Keyframing\"\n        title=\"Keyframing\"\n        src=\"/static/f21c0ce03b325ddd84f2b532eff69e05/13ae7/Keyframing.png\"\n        srcset=\"/static/f21c0ce03b325ddd84f2b532eff69e05/942f4/Keyframing.png 88w,\n/static/f21c0ce03b325ddd84f2b532eff69e05/4edbd/Keyframing.png 175w,\n/static/f21c0ce03b325ddd84f2b532eff69e05/13ae7/Keyframing.png 350w,\n/static/f21c0ce03b325ddd84f2b532eff69e05/52211/Keyframing.png 525w,\n/static/f21c0ce03b325ddd84f2b532eff69e05/b6a9b/Keyframing.png 530w\"\n        sizes=\"(max-width: 350px) 100vw, 350px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span><figcaption>Clustering of points and keyframing</figcaption></figure>\n<figure><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 350px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/a43d96f38e02d70fbe1e6ff685daae6f/b06ae/RejectionSampling.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 96.59090909090908%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAATCAIAAAAf7rriAAAACXBIWXMAAAsSAAALEgHS3X78AAACMUlEQVQ4y51TW3OaQBjl93c603/Ql751WmeaaWJskzAG5aIxRixeMFwkiBREBFSUi/QYpkYeGjM9Dzvf7n5nv7NnvyUEQYjjOPuL3ZuBZKJWq2232+y/QNA0fSC7rmsYxuwZtm0jNk0zj/PRcRzLsiYTw5gYCApkRVEqlUq5XCZJstVq9Xo9juMajQbDMEhjWRbrgtBbrgLHsyRJKpB1XR8MBuCMRiNVVSEE8XA4xDQPRFF8etKTNFklgaZpBTLm7XYbR/b7fRiJGMpzDg7FERg7nY4iK1mwGqtqgSzLcrPZhE4s1ut1eCnLEsvulUNzLh4JPM97v02cVSDDj+l06jwDDnmet3AXgDNzjrH30tyjQD5+bYy2YwkjfmbP3vRUB9o22m7CDXfP/mTPWY7xFh5y0l0axdFxIxXIotpTxlKWZrZtZbts8MhPTEXTxqDNHbfJ01fMue/7hxov5MBfXlbPbrnrxgMdbfcV2hQlMuw9TcdpqirqDV2p0tfL5bIge7PZIIqiCA9Ta92w7XqapFgJwxBPne+i4HRqrlfrwA+wEsfJS2XIwDZ+SBInSZS80s/zxQJGkMwP3I5AA05MnWZoSZFe4eQOpWlGXny9Ir+rmgpTiNZd667TYB+o9pBFkxzM+Bd8Z64p43Ad7mWjbcaqNnzsD0d9d+6e/NJIwO3mwNQkqtUqOulkwQPgpayLn758pG4viVKphNbFN/h1Ct1uFx+GoqjS+ef3H96dXXz7A8eeE0I/1PtTAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"RejectionSampling\"\n        title=\"RejectionSampling\"\n        src=\"/static/a43d96f38e02d70fbe1e6ff685daae6f/13ae7/RejectionSampling.png\"\n        srcset=\"/static/a43d96f38e02d70fbe1e6ff685daae6f/942f4/RejectionSampling.png 88w,\n/static/a43d96f38e02d70fbe1e6ff685daae6f/4edbd/RejectionSampling.png 175w,\n/static/a43d96f38e02d70fbe1e6ff685daae6f/13ae7/RejectionSampling.png 350w,\n/static/a43d96f38e02d70fbe1e6ff685daae6f/52211/RejectionSampling.png 525w,\n/static/a43d96f38e02d70fbe1e6ff685daae6f/b06ae/RejectionSampling.png 560w\"\n        sizes=\"(max-width: 350px) 100vw, 350px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span><figcaption>Rejection Sampling to relearn constrained distributions.</figcaption></figure></p>\n<h4>Culling</h4>\n<p>Once a dense sequence of keyframes is produce, the intermediate keyframe density is reduced to generate a more sparse model. It is important to note that keyframes representing when the set of applied constraints change are kept. </p>\n<p><figure><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 350px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/df0bc26b4b50a7998015134c3fac7e62/89a37/Culling.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 100%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAIAAAAC64paAAAACXBIWXMAAAsSAAALEgHS3X78AAAB3UlEQVQ4y52UbXeaMBTH+f5fZmcvdO2LbaUVBecYnuwEkQdJBDKfq05DEOiuxLN2LaU9uy8ChPzyvw+5UTzPO51OD/9liqqqnPPaf1mWUUrDF0YIgfkoipRut5um6TOsLEsYl8tlkiTyU1qe5zAKIYq8CIJA0TTtNXi73TqOo+u6YRimaQ4GA62y9XpN7j0/9JrgzWYDTs5ms/l8vlgs4CWOYxA8HI+i/E1o2ATvdjuEUKfTwRhLWYgW5ouiYJtsQmgTDO5BViBy0ARlkF2tVhJO1mkwaVTe7/eu6zHG4srki8w2S6Lx2KmHn+5TSZ3zDMcBxqIyeIDAc1iWRMr2TFsf+iGZ5q+con9gicHGh8MBnPyO3AHyp1GS8hRhZNkWKP9d9gIuSjZjdEpFKsqLzxdHIha1e+2hPZS718ATOvmofnB8R64oH+kzn7DYHmNIeD1smIaFLchGTd4qfQinfdf2qCdnHmGoJHRCQw9J3hr9uNY+8SO/KHN+hNrJ3irKorkNoWB0SqSeove62CXXnWGWiXe2cZad/CA4u313e3N18+2nGz28pfnkzO+/GvjXfKm0Wq3RaMyhUYTg6dsGedlt7z8bNnLC801yq6r9fh9uhd67DALVWldfEHb/AOTRY8jWFezjAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Culling\"\n        title=\"Culling\"\n        src=\"/static/df0bc26b4b50a7998015134c3fac7e62/13ae7/Culling.png\"\n        srcset=\"/static/df0bc26b4b50a7998015134c3fac7e62/942f4/Culling.png 88w,\n/static/df0bc26b4b50a7998015134c3fac7e62/4edbd/Culling.png 175w,\n/static/df0bc26b4b50a7998015134c3fac7e62/13ae7/Culling.png 350w,\n/static/df0bc26b4b50a7998015134c3fac7e62/52211/Culling.png 525w,\n/static/df0bc26b4b50a7998015134c3fac7e62/89a37/Culling.png 532w\"\n        sizes=\"(max-width: 350px) 100vw, 350px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span><figcaption>Sparsification step that maintains constraint boundary keyframes.</figcaption></figure></p>\n<h4>Reconstruct Skill</h4>\n<p>Finally a skill is reconstructed by sampling in-sequence constraint-compliant waypoints from each keyframe distribution. Motion planning methods are then used to traverse from waypoint to waypoint.</p>\n<p><figure><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 350px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/e46d2629a5f9b3ab75f1dc6f7aa4bde2/664c8/SkillExecution.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 100%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAIAAAAC64paAAAACXBIWXMAAAsSAAALEgHS3X78AAABb0lEQVQ4y+2TTW+CQBCG/e09NE2b/gxvHkxsIqSABAT6wccBI0sKrYkFa4EEpQHdBe3Y1dbaJhoPPXUOk83MPpl3ZmdrruuWZbk8ymqSJBFCjoRVVT0eVhTlH/6yxcaqqgL/J5XzPOc4zrIs0zQ1TZNlOU1TqmU/jDGGzev3++ARQrZtHwRvp9N00vPM8esLxgTa3gPTXBzHsHagFqpVy8U0m3blrm4YcRLt8N9gXM5uDEXsiFEUvWXZ02AwtO0HhPKiIKQUNNb10DZPYQwnSPPitdm7pwnQOZ/P4zAkGG90VTzPPw+DT34Fw5eEq6Io+r4PIfpDHcdhWZZhWUmWoQXac1HMGPFqkq2HV1MVdUZyTmJ875EWpHWglyRJgiAYhSEc6LZAfDQOOh1hLdswjDv9VrfuaM1qYzuDpUE6HcPUXc9Zwa1WC1T9fIbfF3a5+PDV6eUJclCtXq83Gg3Yp3a7zRxgcE3ghfOzi2az+Q7ThGxJOSKk8QAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"SkillExecution\"\n        title=\"SkillExecution\"\n        src=\"/static/e46d2629a5f9b3ab75f1dc6f7aa4bde2/13ae7/SkillExecution.png\"\n        srcset=\"/static/e46d2629a5f9b3ab75f1dc6f7aa4bde2/942f4/SkillExecution.png 88w,\n/static/e46d2629a5f9b3ab75f1dc6f7aa4bde2/4edbd/SkillExecution.png 175w,\n/static/e46d2629a5f9b3ab75f1dc6f7aa4bde2/13ae7/SkillExecution.png 350w,\n/static/e46d2629a5f9b3ab75f1dc6f7aa4bde2/664c8/SkillExecution.png 524w\"\n        sizes=\"(max-width: 350px) 100vw, 350px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span><figcaption>Exeuction of a constrained skill</figcaption></figure></p>\n<h2>Limitations &#x26; Future Work</h2>\n<p>A limitation of CC-LfD is it’s reliance on unconstrained motion planning. If an environment change introducing an intermediate keyframe occlusion, the motion planners cannot rely on the density of sequential keyframes to maintain constraint adherance. Our guess as to why constraint copmliance does occur in motion planning in the first place is because the density of keyframes effectively restricts the space of feasible motion plans to likely sit within the same space as constraint-compliant motion plans. This is also because the demonstration data to produce these keyframes generally show constraint-compliant paths. </p>\n<p>However, with occluded keyframes, the motion planning algorithms need to diverge away from the initial demonstration trajectory distributions in order to produce a collision-free motion plan. This divergence suddenly exposes the constaint naievete of unconstrained motion planners. Future work will look into integrating constrained motion planners into CC-LfD.</p>\n<h2>References</h2>\n<p><ol><li><b>Robust Robot Learning from Demonstration and Skill Repair Using Conceptual Constraints</b> <br>Mueller, C., Venicx, J. and Hayes, B., 2018. 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 6029--6036. </li><li><b>Recent advances in robot learning from demonstration</b> <br>Ravichandar, H., Polydoros, A.S., Chernova, S. and Billard, A., 2020. Annual Review of Control, Robotics, and Autonomous Systems, Vol 3. Annual Reviews.</li><li><b>Keyframe-based learning from demonstration</b> <br>Akgun, B., Cakmak, M., Jiang, K. and Thomaz, A.L., 2012. International Journal of Social Robotics, Vol 4(4), pp. 343--355. Springer.</li><li><b>Robust Robot Learning from Demonstration and Skill Repair Using Conceptual Constraints</b> <br>Mueller, C., Venicx, J. and Hayes, B., 2018. 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 6029--6036. </li></ol></bibliography></p>\n<h5>Footnotes</h5>\n\n      <div class=\"footnotes\">\n        <hr/>\n        <ol >\n    \n    <li class=\"footnote-list-item\" id=\"fn-1\" >\n          \n        \n      <a href=\"#fnref-1\" class=\"footnote-backref\" style=\"display:inline;text-decoration: none;\">\n        ↩\n      </a>\n    <p class=\"footnote-paragraph\" style=\"display:inline; margin-left: 5px;\">In kinesthetic demonstration, a human operator physically manipulates the robot to enable its sensors to capture a trajectory. In teleoperation, the operator utilizes a remote control device to manipulate the robot. In passive observation, the robot’s external sensors observe a human demonstrating the task. This last method introduces something called the <strong>correspondence problem</strong>: how does one map observational demonstration data to robot control state data?</p>\n      </li>\n      \n    \n\n    <li class=\"footnote-list-item\" id=\"fn-2\" >\n          \n        \n      <a href=\"#fnref-2\" class=\"footnote-backref\" style=\"display:inline;text-decoration: none;\">\n        ↩\n      </a>\n    <p class=\"footnote-paragraph\" style=\"display:inline; margin-left: 5px;\">CC-LfD falls under the policy learning category, where the constrained acyclic directed keyframe graph generates a rough motion plan for the robot to follow. The graph model represents a policy that dictates robot behavior.</p>\n      </li>\n      \n    </ol></div>","fields":{"slug":"/research/cc-lfd/"},"frontmatter":{"authors":["Carl Mueller"],"date":"October 26, 2018","title":"Concept Constrained Learning from Demonstration","coverDesc":"Sawyer hoping to avoid spilling coffee...","featuredImage":{"childImageSharp":{"fluid":{"base64":"data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAkABQDASIAAhEBAxEB/8QAGAABAAMBAAAAAAAAAAAAAAAAAAMEBQL/xAAXAQEAAwAAAAAAAAAAAAAAAAABAAID/9oADAMBAAIQAxAAAAGG/i6I8oEOJ69kvmrBrtSGOkYJ/8QAHxAAAgICAQUAAAAAAAAAAAAAAQIDEQASBBMhIiMx/9oACAEBAAEFAurrgPhyKV1+I3rkouzClkGhcglDcQbaTjtuRlds/8QAFxEAAwEAAAAAAAAAAAAAAAAAEBEgIf/aAAgBAwEBPwFDY//EABgRAAIDAAAAAAAAAAAAAAAAAAARARAh/9oACAECAQE/ARmDi//EACEQAAECBQUBAAAAAAAAAAAAAAEAEQIQITFhAxIgQXGx/9oACAEBAAY/AmIdReIDtpROqWVmwEx06/ZERXwgCCyO2o4f/8QAHBABAAMBAAMBAAAAAAAAAAAAAQARITFBUfCB/9oACAEBAAE/IduI3ZN8+3AgrG69zRMq+xS9dFX+xqS+lzYpV4RLJg+VGlcjQJbcHV8QlBTYAsCWrYmz/9oADAMBAAIAAwAAABBLMDwL7//EABcRAQEBAQAAAAAAAAAAAAAAAAEAERD/2gAIAQMBAT8QcG2SICxnn//EABgRAAMBAQAAAAAAAAAAAAAAAAABEBEx/9oACAECAQE/ENjYELh//8QAHhABAQADAQACAwAAAAAAAAAAAREAIUExYXGBkdH/2gAIAQEAAT8QprlaA7DmMyx8Uu0YIsqTBp5iQBV2DWjAGcEOgTBBQAFXzBJm1AN8v9y0BKlELVTTBZAlQSwvmOd70UfjBHXjODQfoyrAIbsvMkR+cLD7HCaTP//Z","aspectRatio":0.56,"src":"/static/ae38715e04c7da54e072d869a8449e29/58f33/Placement.jpg","srcSet":"/static/ae38715e04c7da54e072d869a8449e29/0caaa/Placement.jpg 140w,\n/static/ae38715e04c7da54e072d869a8449e29/3e0e0/Placement.jpg 280w,\n/static/ae38715e04c7da54e072d869a8449e29/58f33/Placement.jpg 561w,\n/static/ae38715e04c7da54e072d869a8449e29/7b79b/Placement.jpg 841w,\n/static/ae38715e04c7da54e072d869a8449e29/34847/Placement.jpg 1121w,\n/static/ae38715e04c7da54e072d869a8449e29/2eecf/Placement.jpg 1256w","sizes":"(max-width: 561px) 100vw, 561px","maxHeight":1000,"maxWidth":561}}}}}},"pageContext":{"slug":"/research/cc-lfd/"}},"staticQueryHashes":["3048952670","4224293195"]}