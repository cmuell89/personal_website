{"componentChunkName":"component---src-templates-blog-post-js","path":"/research/cc-lfd/","result":{"data":{"markdownRemark":{"id":"fdc4f90d-32ea-59c4-9749-1e36e6f5d2fa","html":"<h2>Why is this important?</h2>\n<p>My PhD researchâ€™s seminal work is called <strong>Concept Constrained Learning from Demonstration</strong> <span id=\"citation-0\" data-hover=\"\"><span class=\"citation-number\">[1]</span></span>. It is this work that motivates my current research into constrained motion planning and human-robot interfaces, specifically augmented reality interfaces. Ultimately, I want to figure out how to make robots easily trainable and usable by non-roboticists. Physical automation is a wide-open frontier in the world of information technology. However, the introduction of collaborative robots into human environments presents a number of challenges often not required of large-scale industrial robots: safety in shared workspaces, rapidly changing task requirements, decision-making, and, perhaps most challenging, adhering to human expectations of behavior. As such, the foundational motivation behind this work is to provide human users the means to easily train collaborative robots to execute dynamic skills while adhering to important behavioral restrictions.</p>\n<h2>Background</h2>\n<h3>Learning from Demonstration</h3>\n<p>Robot Learning from Demonstration (LfD) consists of methods that attempt to learn successful robot behavior models from human input. A human operator interacts with a robotic system through some mode of demonstration, usually through kinesthetic demonstration (e.g. physical interaction), teleoperation (e.g. remote control), or passive observation (e.g. motion tracking observation). Demonstration ideally communicates information about the nature of the skill that the robotic learning system uses to build a learned model that resembles some latent (i.e. hidden) ground truth model. The methods by which robotic systems learn such models spans across the spectrum of machine learning. However there are three broad cateogrizations for robot LfD systems: 1) plan learning, 2) functional optimization, and 3) policy learning  <span id=\"citation-0\" data-hover=\"\"><span class=\"citation-number\">[2]</span></span>.</p>\n<p><figure><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 450px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/4d3b3c608170205bce549ef23656749a/3acf0/ActionShot.jpg\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 55.75221238938053%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAALABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAAAAMCBP/EABYBAQEBAAAAAAAAAAAAAAAAAAABAv/aAAwDAQACEAMQAAABtKW805h//8QAHBAAAgEFAQAAAAAAAAAAAAAAAgMTAAEREiIy/9oACAEBAAEFAm9U8hiWBajfLHAMI+f/xAAWEQEBAQAAAAAAAAAAAAAAAAAAARH/2gAIAQMBAT8Bka//xAAWEQADAAAAAAAAAAAAAAAAAAAAARH/2gAIAQIBAT8BbhD/xAAaEAACAwEBAAAAAAAAAAAAAAAAAQIRIRKh/9oACAEBAAY/AuBQXpsRpl1tCP/EABwQAQACAgMBAAAAAAAAAAAAAAEAESExQVGBsf/aAAgBAQABPyF0e0cvqvEZZ8uKOIzywv5GW6GY3cmJ/9oADAMBAAIAAwAAABBrH//EABcRAAMBAAAAAAAAAAAAAAAAAAABEVH/2gAIAQMBAT8QoqRh/8QAFxEAAwEAAAAAAAAAAAAAAAAAAAERQf/aAAgBAgEBPxC6Dd6f/8QAGxABAAMBAAMAAAAAAAAAAAAAAQARIVExQXH/2gAIAQEAAT8QoIrI6/dm3J2eB18seg2Oo4qsj4hVOagPkRT1DGABg1P/2Q=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"ActionShot\"\n        title=\"ActionShot\"\n        src=\"/static/4d3b3c608170205bce549ef23656749a/20e5d/ActionShot.jpg\"\n        srcset=\"/static/4d3b3c608170205bce549ef23656749a/30b1b/ActionShot.jpg 113w,\n/static/4d3b3c608170205bce549ef23656749a/863e1/ActionShot.jpg 225w,\n/static/4d3b3c608170205bce549ef23656749a/20e5d/ActionShot.jpg 450w,\n/static/4d3b3c608170205bce549ef23656749a/512c0/ActionShot.jpg 675w,\n/static/4d3b3c608170205bce549ef23656749a/8e1fc/ActionShot.jpg 900w,\n/static/4d3b3c608170205bce549ef23656749a/3acf0/ActionShot.jpg 2000w\"\n        sizes=\"(max-width: 450px) 100vw, 450px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span><figcaption>A human user (me) kinestheticaly demonstrating a skill.</figcaption></figure></p>\n<p>The ultimate goal of these learning methods is to facilitate the transfer of information from a non-roboticist, with some expert intuition about the skill, to the robotic learning system. This information is then used by robot skill learning methods to produce successful learned models of the task. Plan learning methods attempt to learn models that operate at high levels of task abstraction, either learning a primitive sequence or hierarchy. Functional optimization methods either directly optimize a candidate trajectory (potentially one derived from demonstration) using a known objective function, or they attempt to learn an objective from demonstration. These approaches often emulate or directly draw from Reinforcement Learning and Inverse Reinforcement Learning techniques. Lastly, policy learning methods produce models that output either trajectories or low-level actions directly.</p>\n<h3>Keyframe Learning from Demonstration</h3>\n<p>CC-LfD is an augnmentation of a learning method called Keyframe LfD (KLfD) <span id=\"citation-0\" data-hover=\"\"><span class=\"citation-number\">[3]</span></span>. In traditional KLfD, human operators teach a skill by providing distinct waypoints of robot state data. This represents a coarse trajectory for the robot to follow. This approach is powerful because it very easily allows users to specify robot motion, but it is somewhat brittle as the learned skill is really a concrete instantiation of one robot trajectory. Any variation to the environment or to changes in user expecations cannot be accomodated.</p>\n<p>Keyframe LfD can be make more robust through automating keyframe generation and through statistical learning. To automate this approach, users first provide high-rate-of-sampling demosntration trajectories of the skill, ideally expressing subtle variation. Demonstration trajectories are aligned using a technique called Dynamic Time Warping <span id=\"citation-0\" data-hover=\"\"><span class=\"citation-number\">[?]</span></span>, which is an algorithmic method to align similar-structural regions in sequential data from one sequence to another. The data points of these temporally aligned demonstration trajectories are clustered into sequential groups across demonstrations. These clusters of robot state data are fitted to learned <em>keyframe distributions</em>, which are used to generate waypoints that the robot follows sequentially to perform a skill. Forming statistical distributions to represent keyframes, as opposed to single points, enables the LfD algorithm to adapt to behavioral restrictions the human operator migth decide to place on the robot.</p>\n<h2>Concept Constrained Learning from Demonstration</h2>\n<p><ol><li><b>Robust Robot Learning from Demonstration and Skill Repair Using Conceptual Constraints</b> <br>Mueller, C., Venicx, J. and Hayes, B., 2018. 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 6029--6036. </li><li><b>Recent advances in robot learning from demonstration</b> <br>Ravichandar, H., Polydoros, A.S., Chernova, S. and Billard, A., 2020. Annual Review of Control, Robotics, and Autonomous Systems, Vol 3. Annual Reviews.</li><li><b>Keyframe-based learning from demonstration</b> <br>Akgun, B., Cakmak, M., Jiang, K. and Thomaz, A.L., 2012. International Journal of Social Robotics, Vol 4(4), pp. 343--355. Springer.</li></ol></bibliography></p>","fields":{"slug":"/research/cc-lfd/"},"frontmatter":{"authors":["Carl Mueller"],"date":"October 26, 2018","title":"Concept Constrained Learning from Demonstration","coverDesc":"Sawyer hoping to avoid spilling coffee...","featuredImage":{"childImageSharp":{"fluid":{"base64":"data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAkABQDASIAAhEBAxEB/8QAGAABAAMBAAAAAAAAAAAAAAAAAAMEBQL/xAAXAQEAAwAAAAAAAAAAAAAAAAABAAID/9oADAMBAAIQAxAAAAGG/i6I8oEOJ69kvmrBrtSGOkYJ/8QAHxAAAgICAQUAAAAAAAAAAAAAAQIDEQASBBMhIiMx/9oACAEBAAEFAurrgPhyKV1+I3rkouzClkGhcglDcQbaTjtuRlds/8QAFxEAAwEAAAAAAAAAAAAAAAAAEBEgIf/aAAgBAwEBPwFDY//EABgRAAIDAAAAAAAAAAAAAAAAAAARARAh/9oACAECAQE/ARmDi//EACEQAAECBQUBAAAAAAAAAAAAAAEAEQIQITFhAxIgQXGx/9oACAEBAAY/AmIdReIDtpROqWVmwEx06/ZERXwgCCyO2o4f/8QAHBABAAMBAAMBAAAAAAAAAAAAAQARITFBUfCB/9oACAEBAAE/IduI3ZN8+3AgrG69zRMq+xS9dFX+xqS+lzYpV4RLJg+VGlcjQJbcHV8QlBTYAsCWrYmz/9oADAMBAAIAAwAAABBLMDwL7//EABcRAQEBAQAAAAAAAAAAAAAAAAEAERD/2gAIAQMBAT8QcG2SICxnn//EABgRAAMBAQAAAAAAAAAAAAAAAAABEBEx/9oACAECAQE/ENjYELh//8QAHhABAQADAQACAwAAAAAAAAAAAREAIUExYXGBkdH/2gAIAQEAAT8QprlaA7DmMyx8Uu0YIsqTBp5iQBV2DWjAGcEOgTBBQAFXzBJm1AN8v9y0BKlELVTTBZAlQSwvmOd70UfjBHXjODQfoyrAIbsvMkR+cLD7HCaTP//Z","aspectRatio":0.56,"src":"/static/ae38715e04c7da54e072d869a8449e29/58f33/Placement.jpg","srcSet":"/static/ae38715e04c7da54e072d869a8449e29/0caaa/Placement.jpg 140w,\n/static/ae38715e04c7da54e072d869a8449e29/3e0e0/Placement.jpg 280w,\n/static/ae38715e04c7da54e072d869a8449e29/58f33/Placement.jpg 561w,\n/static/ae38715e04c7da54e072d869a8449e29/7b79b/Placement.jpg 841w,\n/static/ae38715e04c7da54e072d869a8449e29/34847/Placement.jpg 1121w,\n/static/ae38715e04c7da54e072d869a8449e29/2eecf/Placement.jpg 1256w","sizes":"(max-width: 561px) 100vw, 561px","maxHeight":1000,"maxWidth":561}}}}}},"pageContext":{"slug":"/research/cc-lfd/"}},"staticQueryHashes":["3048952670","4224293195"]}